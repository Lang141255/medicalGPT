{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DzDQBxuCacpz"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "dT3F583Xacp0"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DryeHjAcacp1"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QmVyW5D-acp1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjGQXNZmacp1"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cvEJnPIpacp1",
        "outputId": "4fbf168a-f1c4-403a-f960-3eaa6eed67f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MedicalGPT' already exists and is not an empty directory.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   README.md\n",
            "chatpdf.py                  requirements.txt\n",
            "CITATION.cff                reward_modeling.py\n",
            "_config.yml                 \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "CONTRIBUTING.md             run_dpo.sh\n",
            "convert_dataset.py          run_eval_quantize.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_full_sft.sh\n",
            "DISCLAIMER                  run_grpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_orpo.sh\n",
            "dpo_training.py             run_ppo.sh\n",
            "eval_quantize.py            run_pt.sh\n",
            "fastapi_server_demo.py      run_quant.sh\n",
            "gradio_demo.py              run_rm.sh\n",
            "grpo_training.py            run_sft_accelerate.sh\n",
            "inference_multigpu_demo.py  run_sft.sh\n",
            "inference.py                run_training_dpo_pipeline.ipynb\n",
            "LICENSE                     run_training_ppo_pipeline.ipynb\n",
            "\u001b[01;34mMedicalGPT\u001b[0m/                 supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "\u001b[01;34moutputs-pt-v1\u001b[0m/              zero2.json\n",
            "ppo_training.py             zero2.yaml\n",
            "pretraining.py              zero3.json\n",
            "README_EN.md                zero3.yaml\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.7.3)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.3)\n",
            "Requirement already satisfied: trl>=0.15.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.26.2)\n",
            "Requirement already satisfied: latex2sympy2_extended in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (1.0.6)\n",
            "Requirement already satisfied: math-verify==0.5.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.5.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTSihMCFacp2"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Zuhm9QOacp2",
        "outputId": "0de9ebd8-6ab8-4670-f5df-2a7cf5b89c4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J60mRSC2acp2",
        "outputId": "0f845603-8cc6-4db3-8c73-8fdb86453479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 05:38:34.726302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766295514.746307    6483 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766295514.752194    6483 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766295514.767272    6483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766295514.767297    6483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766295514.767301    6483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766295514.767305    6483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 05:38:34.771919: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-12-21 05:38:41.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Dec21_05-38-41_dd747642cfe0,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m476\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt']\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1meval files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt']\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:41.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m518\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m581\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2501\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m582\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m583\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
            "limit to the factories, forges, refineries, and railways that could be\n",
            "built, to the multitudes that could be employed in conquering a\n",
            "continent. As for the future, that was in the hands of Providence!\n",
            "\n",
            "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
            "and the planters of Calhoun's had their theories of government and\n",
            "politics, so the leaders in business enterprise had theirs. It was\n",
            "simple and easily stated. \"It is the duty of the government,\" they\n",
            "ur\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m595\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m596\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:46.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m597\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
            "limit to the factories, forges, refineries, and railways that could be\n",
            "built, to the multitudes that could be employed in conquering a\n",
            "continent. As for the future, that was in the hands of Providence!\n",
            "\n",
            "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
            "and the planters of Calhoun's had their theories of government and\n",
            "politics, so the leaders in business enterprise had theirs. It was\n",
            "simple and easily stated. \"It is the duty of the government,\" they\n",
            "ur\u001b[0m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[32m2025-12-21 05:38:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m661\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m674\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m675\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/content/MedicalGPT/pretraining.py:705: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2025-12-21 05:38:47.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m720\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-21 05:38:48.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 99805,  52801,   3837,  99727, 102461,   3837, 105665,  75882,  39165,\n",
            "          99500,  42411, 109565,  99306,  52801,  81264,  99727, 102461, 105626,\n",
            "          44793,  36987,  37474, 106292, 104139, 115131,   3837,  97639,  35926,\n",
            "          39165, 110121,   3837,  99245,  99500,  16530,  99500,   9370,   3837,\n",
            "          97639,  99190, 112471,   9370, 101453,  81596,  81264,  99727, 104000,\n",
            "          44793,  36987, 103924, 120671,   3837,  56568, 104298, 100457,  99486,\n",
            "          36993, 100219,  52801, 104389,   3837, 100224,  99925, 112730,  36587,\n",
            "         108386,   3837, 108967, 100250,  32945,  37474,  99783, 106052,  36987,\n",
            "         106665,  99295,  16744,   3837, 109739, 117350, 118459,   3837,  77540,\n",
            "          90885, 100141, 102513,   1773,  99727, 102461, 103856, 103856,   3837,\n",
            "          35946, 105189,  49187, 103929,  64272, 111241,  17447, 100080, 101477,\n",
            "           3837,  39973,  98650,  63109, 111520,  99315, 106433,   1773,  99172,\n",
            "         112720,  11622, 100672, 112946,  36407, 110327,  14777, 100036,   3837,\n",
            "         111566, 104501,  89012, 104269,  26288, 109503, 100228, 100215, 100815,\n",
            "         120493,   3837],\n",
            "        [100037,   3837,  99172, 107541,  33447, 103947, 102009,   3837,  77288,\n",
            "          89012, 102987, 104609,  34187,   3837, 107585,  74763, 103036,  16530,\n",
            "          99694,   3837, 110253,  11622, 101270,  32555,  47534,   3837,  31843,\n",
            "          47534, 110253,  99723,  99234,   9370,  99632,  20726,   8997,  99718,\n",
            "          15946, 105501,  91680,  99353,  75061,  99677, 100037,  17447, 116345,\n",
            "         106701,  31843,  47534,   3837, 104578,  99518,  45181,  99283, 100037,\n",
            "          52526,  20221,   3837, 108954,  43288,  30709, 114049,  99261, 100007,\n",
            "          18830, 101073, 103524,  31843,  47534,   3837, 100538, 106097,   3837,\n",
            "          52801,  18493,  99283, 103366,  17447,  31843,  47534,  99632,  20726,\n",
            "           3837, 106911,  99786,  18830, 104361,   3837,  99795, 107585,  99744,\n",
            "         100209,  53222,  99694,  75061,  99677,  99336, 121601,   3837, 106429,\n",
            "          18830,  85336,  42192,  36407,   1773,  75061,  31207, 100956,  49567,\n",
            "          74763,  99639, 106148, 111038,   3837, 104138, 104219, 105187,   3837,\n",
            "         105777,  99786,  99236,  99745,  99236,  99378,   3837, 106682, 111157,\n",
            "          52510, 107691],\n",
            "        [ 75405, 106783,  79766,  44793,  36987,  56568,  99882, 101553, 100469,\n",
            "           3837,  35946,  99364,  16530, 100469,  32945, 107279,  72225, 104853,\n",
            "          27091,  99710,   8997,  37474,  99783, 104639,  99517,  79766,  99686,\n",
            "           9370,  36629,  99225,   3837,  99518,  99639, 104660,   3837, 103961,\n",
            "         101920,   3837, 101317,  15946, 105218, 110963,   8903,   9370, 105748,\n",
            "           3837, 113235,   2073, 103924, 111447,    854, 104494,   3837,  99882,\n",
            "         108158,   1773,  43288,  99854, 105748,  99364,  29524, 104801,  30709,\n",
            "         101953,  18493, 102766, 101317,  15946, 102363, 113698,  27733,   3837,\n",
            "          44063,  42411, 100859,  44729,  14777, 101490, 101490,   9370, 100958,\n",
            "          63789,   1773,  37474,  99783, 105777,  59879,  99296, 105925,   3837,\n",
            "         113201, 100868, 100088,  99364,  29524,  99789,  99955,  99791,  14777,\n",
            "         101425, 101425, 102914,  99898,   8997,  75405, 106783,  79766,  99851,\n",
            "          44793,  36987,  56568,  14053, 109111, 104060,  81264,  37474,  99783,\n",
            "         119332, 108375,  44793,  36987,  43288,  14053,  43288,  63789, 100859,\n",
            "          99632,  14053]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 99805,  52801,   3837,  99727, 102461,   3837, 105665,  75882,  39165,\n",
            "          99500,  42411, 109565,  99306,  52801,  81264,  99727, 102461, 105626,\n",
            "          44793,  36987,  37474, 106292, 104139, 115131,   3837,  97639,  35926,\n",
            "          39165, 110121,   3837,  99245,  99500,  16530,  99500,   9370,   3837,\n",
            "          97639,  99190, 112471,   9370, 101453,  81596,  81264,  99727, 104000,\n",
            "          44793,  36987, 103924, 120671,   3837,  56568, 104298, 100457,  99486,\n",
            "          36993, 100219,  52801, 104389,   3837, 100224,  99925, 112730,  36587,\n",
            "         108386,   3837, 108967, 100250,  32945,  37474,  99783, 106052,  36987,\n",
            "         106665,  99295,  16744,   3837, 109739, 117350, 118459,   3837,  77540,\n",
            "          90885, 100141, 102513,   1773,  99727, 102461, 103856, 103856,   3837,\n",
            "          35946, 105189,  49187, 103929,  64272, 111241,  17447, 100080, 101477,\n",
            "           3837,  39973,  98650,  63109, 111520,  99315, 106433,   1773,  99172,\n",
            "         112720,  11622, 100672, 112946,  36407, 110327,  14777, 100036,   3837,\n",
            "         111566, 104501,  89012, 104269,  26288, 109503, 100228, 100215, 100815,\n",
            "         120493,   3837],\n",
            "        [100037,   3837,  99172, 107541,  33447, 103947, 102009,   3837,  77288,\n",
            "          89012, 102987, 104609,  34187,   3837, 107585,  74763, 103036,  16530,\n",
            "          99694,   3837, 110253,  11622, 101270,  32555,  47534,   3837,  31843,\n",
            "          47534, 110253,  99723,  99234,   9370,  99632,  20726,   8997,  99718,\n",
            "          15946, 105501,  91680,  99353,  75061,  99677, 100037,  17447, 116345,\n",
            "         106701,  31843,  47534,   3837, 104578,  99518,  45181,  99283, 100037,\n",
            "          52526,  20221,   3837, 108954,  43288,  30709, 114049,  99261, 100007,\n",
            "          18830, 101073, 103524,  31843,  47534,   3837, 100538, 106097,   3837,\n",
            "          52801,  18493,  99283, 103366,  17447,  31843,  47534,  99632,  20726,\n",
            "           3837, 106911,  99786,  18830, 104361,   3837,  99795, 107585,  99744,\n",
            "         100209,  53222,  99694,  75061,  99677,  99336, 121601,   3837, 106429,\n",
            "          18830,  85336,  42192,  36407,   1773,  75061,  31207, 100956,  49567,\n",
            "          74763,  99639, 106148, 111038,   3837, 104138, 104219, 105187,   3837,\n",
            "         105777,  99786,  99236,  99745,  99236,  99378,   3837, 106682, 111157,\n",
            "          52510, 107691],\n",
            "        [ 75405, 106783,  79766,  44793,  36987,  56568,  99882, 101553, 100469,\n",
            "           3837,  35946,  99364,  16530, 100469,  32945, 107279,  72225, 104853,\n",
            "          27091,  99710,   8997,  37474,  99783, 104639,  99517,  79766,  99686,\n",
            "           9370,  36629,  99225,   3837,  99518,  99639, 104660,   3837, 103961,\n",
            "         101920,   3837, 101317,  15946, 105218, 110963,   8903,   9370, 105748,\n",
            "           3837, 113235,   2073, 103924, 111447,    854, 104494,   3837,  99882,\n",
            "         108158,   1773,  43288,  99854, 105748,  99364,  29524, 104801,  30709,\n",
            "         101953,  18493, 102766, 101317,  15946, 102363, 113698,  27733,   3837,\n",
            "          44063,  42411, 100859,  44729,  14777, 101490, 101490,   9370, 100958,\n",
            "          63789,   1773,  37474,  99783, 105777,  59879,  99296, 105925,   3837,\n",
            "         113201, 100868, 100088,  99364,  29524,  99789,  99955,  99791,  14777,\n",
            "         101425, 101425, 102914,  99898,   8997,  75405, 106783,  79766,  99851,\n",
            "          44793,  36987,  56568,  14053, 109111, 104060,  81264,  37474,  99783,\n",
            "         119332, 108375,  44793,  36987,  43288,  14053,  43288,  63789, 100859,\n",
            "          99632,  14053]], device='cuda:0')}\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "{'loss': 4.3649, 'grad_norm': 2.662637710571289, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 3.8427, 'grad_norm': 2.7369015216827393, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.01}\n",
            "{'loss': 3.8684, 'grad_norm': 2.2510342597961426, 'learning_rate': 9.047619047619048e-05, 'epoch': 0.02}\n",
            "{'loss': 3.7237, 'grad_norm': 2.342863082885742, 'learning_rate': 0.0001380952380952381, 'epoch': 0.04}\n",
            "{'loss': 3.7414, 'grad_norm': 2.783025026321411, 'learning_rate': 0.00018571428571428572, 'epoch': 0.05}\n",
            "{'loss': 3.5616, 'grad_norm': 2.780888319015503, 'learning_rate': 0.00019823232323232324, 'epoch': 0.06}\n",
            "  6% 50/834 [00:32<08:24,  1.55it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.68it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.76it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 3.469494581222534, 'eval_accuracy': 0.37086614173228344, 'eval_runtime': 0.7477, 'eval_samples_per_second': 13.374, 'eval_steps_per_second': 5.349, 'epoch': 0.06}\n",
            "  6% 50/834 [00:32<08:24,  1.55it/s]\n",
            "100% 4/4 [00:00<00:00,  7.29it/s]\u001b[A\n",
            "{'loss': 3.6554, 'grad_norm': 2.615678310394287, 'learning_rate': 0.0001957070707070707, 'epoch': 0.07}\n",
            "{'loss': 3.5565, 'grad_norm': 2.670957326889038, 'learning_rate': 0.0001931818181818182, 'epoch': 0.08}\n",
            "{'loss': 3.7362, 'grad_norm': 2.5416817665100098, 'learning_rate': 0.00019065656565656565, 'epoch': 0.1}\n",
            "{'loss': 3.6488, 'grad_norm': 3.268122434616089, 'learning_rate': 0.00018813131313131313, 'epoch': 0.11}\n",
            "{'loss': 3.7145, 'grad_norm': 2.5344998836517334, 'learning_rate': 0.00018560606060606061, 'epoch': 0.12}\n",
            " 12% 100/834 [01:06<08:09,  1.50it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.20it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.50it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.3957297801971436, 'eval_accuracy': 0.3700787401574803, 'eval_runtime': 0.7671, 'eval_samples_per_second': 13.037, 'eval_steps_per_second': 5.215, 'epoch': 0.12}\n",
            " 12% 100/834 [01:07<08:09,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.02it/s]\u001b[A\n",
            "{'loss': 3.5888, 'grad_norm': 2.5746119022369385, 'learning_rate': 0.0001830808080808081, 'epoch': 0.13}\n",
            "{'loss': 3.5134, 'grad_norm': 2.7358458042144775, 'learning_rate': 0.00018055555555555557, 'epoch': 0.14}\n",
            "{'loss': 3.4924, 'grad_norm': 2.326528310775757, 'learning_rate': 0.00017803030303030303, 'epoch': 0.16}\n",
            "{'loss': 3.5109, 'grad_norm': 2.4921517372131348, 'learning_rate': 0.0001755050505050505, 'epoch': 0.17}\n",
            "{'loss': 3.4465, 'grad_norm': 2.4825711250305176, 'learning_rate': 0.000172979797979798, 'epoch': 0.18}\n",
            " 18% 150/834 [01:40<07:28,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.32it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.384777784347534, 'eval_accuracy': 0.38188976377952755, 'eval_runtime': 0.7547, 'eval_samples_per_second': 13.25, 'eval_steps_per_second': 5.3, 'epoch': 0.18}\n",
            " 18% 150/834 [01:41<07:28,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "{'loss': 3.574, 'grad_norm': 2.8006069660186768, 'learning_rate': 0.00017045454545454547, 'epoch': 0.19}\n",
            "{'loss': 3.3414, 'grad_norm': 2.3214573860168457, 'learning_rate': 0.00016792929292929295, 'epoch': 0.2}\n",
            "{'loss': 3.6418, 'grad_norm': 2.7570717334747314, 'learning_rate': 0.0001654040404040404, 'epoch': 0.22}\n",
            "{'loss': 3.4888, 'grad_norm': 2.4599506855010986, 'learning_rate': 0.0001628787878787879, 'epoch': 0.23}\n",
            "{'loss': 3.4044, 'grad_norm': 2.375251054763794, 'learning_rate': 0.00016035353535353536, 'epoch': 0.24}\n",
            " 24% 200/834 [02:14<06:58,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.41it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.3135266304016113, 'eval_accuracy': 0.38818897637795274, 'eval_runtime': 0.7641, 'eval_samples_per_second': 13.087, 'eval_steps_per_second': 5.235, 'epoch': 0.24}\n",
            " 24% 200/834 [02:15<06:58,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "{'loss': 3.5141, 'grad_norm': 2.61372709274292, 'learning_rate': 0.00015782828282828284, 'epoch': 0.25}\n",
            "{'loss': 3.451, 'grad_norm': 3.038224458694458, 'learning_rate': 0.0001553030303030303, 'epoch': 0.26}\n",
            "{'loss': 3.4418, 'grad_norm': 2.2720625400543213, 'learning_rate': 0.00015277777777777777, 'epoch': 0.28}\n",
            "{'loss': 3.5558, 'grad_norm': 2.536780595779419, 'learning_rate': 0.00015025252525252526, 'epoch': 0.29}\n",
            "{'loss': 3.4541, 'grad_norm': 2.3049979209899902, 'learning_rate': 0.00014772727272727274, 'epoch': 0.3}\n",
            " 30% 250/834 [02:48<06:22,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.22it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.25339937210083, 'eval_accuracy': 0.3952755905511811, 'eval_runtime': 0.7586, 'eval_samples_per_second': 13.182, 'eval_steps_per_second': 5.273, 'epoch': 0.3}\n",
            " 30% 250/834 [02:49<06:22,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.5747, 'grad_norm': 2.590599536895752, 'learning_rate': 0.00014520202020202022, 'epoch': 0.31}\n",
            "{'loss': 3.5652, 'grad_norm': 2.6194186210632324, 'learning_rate': 0.00014267676767676767, 'epoch': 0.32}\n",
            "{'loss': 3.3796, 'grad_norm': 2.60723876953125, 'learning_rate': 0.00014015151515151518, 'epoch': 0.34}\n",
            "{'loss': 3.5683, 'grad_norm': 1.9630181789398193, 'learning_rate': 0.00013762626262626263, 'epoch': 0.35}\n",
            "{'loss': 3.5825, 'grad_norm': 2.476257801055908, 'learning_rate': 0.0001351010101010101, 'epoch': 0.36}\n",
            " 36% 300/834 [03:22<05:49,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.06it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.50it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.2452151775360107, 'eval_accuracy': 0.3984251968503937, 'eval_runtime': 0.7659, 'eval_samples_per_second': 13.057, 'eval_steps_per_second': 5.223, 'epoch': 0.36}\n",
            " 36% 300/834 [03:23<05:49,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.298, 'grad_norm': 2.1655807495117188, 'learning_rate': 0.00013257575757575756, 'epoch': 0.37}\n",
            "{'loss': 3.476, 'grad_norm': 2.282831907272339, 'learning_rate': 0.00013005050505050507, 'epoch': 0.38}\n",
            "{'loss': 3.4654, 'grad_norm': 2.462603807449341, 'learning_rate': 0.00012752525252525255, 'epoch': 0.4}\n",
            "{'loss': 3.7164, 'grad_norm': 2.8174808025360107, 'learning_rate': 0.000125, 'epoch': 0.41}\n",
            "{'loss': 3.6148, 'grad_norm': 2.4084041118621826, 'learning_rate': 0.00012247474747474748, 'epoch': 0.42}\n",
            " 42% 350/834 [03:56<05:16,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.34it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.2184596061706543, 'eval_accuracy': 0.40078740157480314, 'eval_runtime': 0.7573, 'eval_samples_per_second': 13.205, 'eval_steps_per_second': 5.282, 'epoch': 0.42}\n",
            " 42% 350/834 [03:57<05:16,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.13it/s]\u001b[A\n",
            "{'loss': 3.3923, 'grad_norm': 2.6720566749572754, 'learning_rate': 0.00011994949494949495, 'epoch': 0.43}\n",
            "{'loss': 3.5187, 'grad_norm': 2.7918663024902344, 'learning_rate': 0.00011742424242424244, 'epoch': 0.44}\n",
            "{'loss': 3.4307, 'grad_norm': 2.476665735244751, 'learning_rate': 0.00011489898989898991, 'epoch': 0.46}\n",
            "{'loss': 3.3192, 'grad_norm': 3.2224559783935547, 'learning_rate': 0.00011237373737373738, 'epoch': 0.47}\n",
            "{'loss': 3.3593, 'grad_norm': 2.5837185382843018, 'learning_rate': 0.00010984848484848484, 'epoch': 0.48}\n",
            " 48% 400/834 [04:30<04:44,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.24it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.21697998046875, 'eval_accuracy': 0.39448818897637794, 'eval_runtime': 0.7569, 'eval_samples_per_second': 13.212, 'eval_steps_per_second': 5.285, 'epoch': 0.48}\n",
            " 48% 400/834 [04:31<04:44,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.16it/s]\u001b[A\n",
            "{'loss': 3.4564, 'grad_norm': 2.187380075454712, 'learning_rate': 0.00010732323232323234, 'epoch': 0.49}\n",
            "{'loss': 3.5155, 'grad_norm': 2.611198902130127, 'learning_rate': 0.0001047979797979798, 'epoch': 0.5}\n",
            "{'loss': 3.4299, 'grad_norm': 2.4081382751464844, 'learning_rate': 0.00010227272727272727, 'epoch': 0.52}\n",
            "{'loss': 3.3984, 'grad_norm': 2.613616943359375, 'learning_rate': 9.974747474747475e-05, 'epoch': 0.53}\n",
            "{'loss': 3.5044, 'grad_norm': 2.788773775100708, 'learning_rate': 9.722222222222223e-05, 'epoch': 0.54}\n",
            " 54% 450/834 [05:04<04:12,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.46it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1700220108032227, 'eval_accuracy': 0.4086614173228346, 'eval_runtime': 0.7666, 'eval_samples_per_second': 13.045, 'eval_steps_per_second': 5.218, 'epoch': 0.54}\n",
            " 54% 450/834 [05:05<04:12,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.4777, 'grad_norm': 2.640551805496216, 'learning_rate': 9.469696969696971e-05, 'epoch': 0.55}\n",
            "{'loss': 3.171, 'grad_norm': 2.7017667293548584, 'learning_rate': 9.217171717171718e-05, 'epoch': 0.56}\n",
            "{'loss': 3.5754, 'grad_norm': 2.4231462478637695, 'learning_rate': 8.964646464646466e-05, 'epoch': 0.58}\n",
            "{'loss': 3.364, 'grad_norm': 2.9380478858947754, 'learning_rate': 8.712121212121212e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4838, 'grad_norm': 2.9490060806274414, 'learning_rate': 8.459595959595959e-05, 'epoch': 0.6}\n",
            " 60% 500/834 [05:38<03:39,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.28it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1483092308044434, 'eval_accuracy': 0.4086614173228346, 'eval_runtime': 0.7577, 'eval_samples_per_second': 13.198, 'eval_steps_per_second': 5.279, 'epoch': 0.6}\n",
            " 60% 500/834 [05:39<03:39,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.16it/s]\u001b[A\n",
            "{'loss': 3.4511, 'grad_norm': 2.3723232746124268, 'learning_rate': 8.207070707070707e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3253, 'grad_norm': 2.6612112522125244, 'learning_rate': 7.954545454545455e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4636, 'grad_norm': 2.629765033721924, 'learning_rate': 7.702020202020203e-05, 'epoch': 0.64}\n",
            "{'loss': 3.4424, 'grad_norm': 2.556492805480957, 'learning_rate': 7.44949494949495e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3787, 'grad_norm': 2.6288695335388184, 'learning_rate': 7.196969696969698e-05, 'epoch': 0.66}\n",
            " 66% 550/834 [06:12<03:08,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.46it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.62it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1326255798339844, 'eval_accuracy': 0.41338582677165353, 'eval_runtime': 0.7536, 'eval_samples_per_second': 13.269, 'eval_steps_per_second': 5.308, 'epoch': 0.66}\n",
            " 66% 550/834 [06:13<03:08,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "{'loss': 3.3266, 'grad_norm': 2.4826407432556152, 'learning_rate': 6.944444444444444e-05, 'epoch': 0.67}\n",
            "{'loss': 3.4087, 'grad_norm': 2.5332326889038086, 'learning_rate': 6.691919191919192e-05, 'epoch': 0.68}\n",
            "{'loss': 3.3455, 'grad_norm': 2.576907157897949, 'learning_rate': 6.439393939393939e-05, 'epoch': 0.7}\n",
            "{'loss': 3.374, 'grad_norm': 2.6851816177368164, 'learning_rate': 6.186868686868687e-05, 'epoch': 0.71}\n",
            "{'loss': 3.3815, 'grad_norm': 2.459540367126465, 'learning_rate': 5.9343434343434345e-05, 'epoch': 0.72}\n",
            " 72% 600/834 [06:46<02:34,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.06it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.48it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.102337598800659, 'eval_accuracy': 0.41338582677165353, 'eval_runtime': 0.7743, 'eval_samples_per_second': 12.915, 'eval_steps_per_second': 5.166, 'epoch': 0.72}\n",
            " 72% 600/834 [06:47<02:34,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.10it/s]\u001b[A\n",
            "{'loss': 3.249, 'grad_norm': 2.5419325828552246, 'learning_rate': 5.6818181818181825e-05, 'epoch': 0.73}\n",
            "{'loss': 3.4794, 'grad_norm': 2.7224905490875244, 'learning_rate': 5.42929292929293e-05, 'epoch': 0.74}\n",
            "{'loss': 3.3907, 'grad_norm': 2.5372188091278076, 'learning_rate': 5.1767676767676765e-05, 'epoch': 0.76}\n",
            "{'loss': 3.1198, 'grad_norm': 2.7062246799468994, 'learning_rate': 4.9242424242424245e-05, 'epoch': 0.77}\n",
            "{'loss': 3.3557, 'grad_norm': 2.884944438934326, 'learning_rate': 4.671717171717172e-05, 'epoch': 0.78}\n",
            " 78% 650/834 [07:20<02:00,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.24it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.55it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.099238872528076, 'eval_accuracy': 0.4094488188976378, 'eval_runtime': 0.7607, 'eval_samples_per_second': 13.145, 'eval_steps_per_second': 5.258, 'epoch': 0.78}\n",
            " 78% 650/834 [07:21<02:00,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.15it/s]\u001b[A\n",
            "{'loss': 3.3082, 'grad_norm': 2.7234199047088623, 'learning_rate': 4.41919191919192e-05, 'epoch': 0.79}\n",
            "{'loss': 3.3178, 'grad_norm': 2.6960668563842773, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.8}\n",
            "{'loss': 3.2785, 'grad_norm': 2.408557653427124, 'learning_rate': 3.9141414141414145e-05, 'epoch': 0.82}\n",
            "{'loss': 3.2969, 'grad_norm': 2.718080520629883, 'learning_rate': 3.661616161616162e-05, 'epoch': 0.83}\n",
            "{'loss': 3.3398, 'grad_norm': 2.1534392833709717, 'learning_rate': 3.409090909090909e-05, 'epoch': 0.84}\n",
            " 84% 700/834 [07:54<01:27,  1.53it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.24it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.56it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1019835472106934, 'eval_accuracy': 0.4094488188976378, 'eval_runtime': 0.7586, 'eval_samples_per_second': 13.183, 'eval_steps_per_second': 5.273, 'epoch': 0.84}\n",
            " 84% 700/834 [07:55<01:27,  1.53it/s]\n",
            "100% 4/4 [00:00<00:00,  7.15it/s]\u001b[A\n",
            "{'loss': 3.4757, 'grad_norm': 2.4204766750335693, 'learning_rate': 3.1565656565656566e-05, 'epoch': 0.85}\n",
            "{'loss': 3.5089, 'grad_norm': 2.6267647743225098, 'learning_rate': 2.904040404040404e-05, 'epoch': 0.86}\n",
            "{'loss': 3.4507, 'grad_norm': 3.0365986824035645, 'learning_rate': 2.6515151515151516e-05, 'epoch': 0.88}\n",
            "{'loss': 3.2887, 'grad_norm': 2.452011823654175, 'learning_rate': 2.398989898989899e-05, 'epoch': 0.89}\n",
            "{'loss': 3.3481, 'grad_norm': 2.5190155506134033, 'learning_rate': 2.1464646464646466e-05, 'epoch': 0.9}\n",
            " 90% 750/834 [08:28<00:55,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.27it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.101022243499756, 'eval_accuracy': 0.4078740157480315, 'eval_runtime': 0.7596, 'eval_samples_per_second': 13.165, 'eval_steps_per_second': 5.266, 'epoch': 0.9}\n",
            " 90% 750/834 [08:29<00:55,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.11it/s]\u001b[A\n",
            "{'loss': 3.3091, 'grad_norm': 3.0233945846557617, 'learning_rate': 1.893939393939394e-05, 'epoch': 0.91}\n",
            "{'loss': 3.3472, 'grad_norm': 2.384373903274536, 'learning_rate': 1.6414141414141416e-05, 'epoch': 0.92}\n",
            "{'loss': 3.2723, 'grad_norm': 2.4281115531921387, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.94}\n",
            "{'loss': 3.1899, 'grad_norm': 2.2629575729370117, 'learning_rate': 1.1363636363636365e-05, 'epoch': 0.95}\n",
            "{'loss': 3.3843, 'grad_norm': 2.8946597576141357, 'learning_rate': 8.838383838383838e-06, 'epoch': 0.96}\n",
            " 96% 800/834 [09:02<00:22,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.25it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.56it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.091337203979492, 'eval_accuracy': 0.4086614173228346, 'eval_runtime': 0.7566, 'eval_samples_per_second': 13.217, 'eval_steps_per_second': 5.287, 'epoch': 0.96}\n",
            " 96% 800/834 [09:03<00:22,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.13it/s]\u001b[A\n",
            "{'loss': 3.2339, 'grad_norm': 2.785813331604004, 'learning_rate': 6.313131313131314e-06, 'epoch': 0.97}\n",
            "{'loss': 3.3134, 'grad_norm': 2.8589587211608887, 'learning_rate': 3.7878787878787882e-06, 'epoch': 0.98}\n",
            "{'loss': 3.4174, 'grad_norm': 2.641127347946167, 'learning_rate': 1.2626262626262627e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 566.5833, 'train_samples_per_second': 4.414, 'train_steps_per_second': 1.472, 'train_loss': 3.4547316993740824, 'epoch': 1.0}\n",
            "100% 834/834 [09:26<00:00,  1.47it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648097GF\n",
            "  train_loss               =     3.4547\n",
            "  train_runtime            = 0:09:26.58\n",
            "  train_samples            =       2501\n",
            "  train_samples_per_second =      4.414\n",
            "  train_steps_per_second   =      1.472\n",
            "\u001b[32m2025-12-21 05:48:15.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m738\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 566.5833, 'train_samples_per_second': 4.414, 'train_steps_per_second': 1.472, 'total_flos': 695888898981888.0, 'train_loss': 3.4547316993740824, 'epoch': 1.0, 'train_samples': 2501}\u001b[0m\n",
            "\u001b[32m2025-12-21 05:48:15.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m739\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-12-21 05:48:16.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m747\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 4/4 [00:00<00:00,  7.34it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.4094\n",
            "  eval_loss               =     3.0909\n",
            "  eval_runtime            = 0:00:00.75\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     13.326\n",
            "  eval_steps_per_second   =      5.331\n",
            "  perplexity              =    21.9977\n",
            "\u001b[32m2025-12-21 05:48:17.182\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m760\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.0909368991851807, 'eval_accuracy': 0.4094488188976378, 'eval_runtime': 0.7504, 'eval_samples_per_second': 13.326, 'eval_steps_per_second': 5.331, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 21.997677930745233}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rzvqt9giacp2",
        "outputId": "6f3352d3-77d1-4745-f202-26f9701e479c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Dec 21 05:48 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 21 05:48 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 21 05:48 added_tokens.json\n",
            "-rw-r--r-- 1 root root  472 Dec 21 05:48 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 05:48 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 05:47 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 05:47 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 05:48 \u001b[01;34mcheckpoint-834\u001b[0m/\n",
            "-rw-r--r-- 1 root root  263 Dec 21 05:48 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 05:48 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Dec 21 05:48 README.md\n",
            "drwxr-xr-x 4 root root 4.0K Dec 21 05:38 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Dec 21 05:48 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.7K Dec 21 05:48 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Dec 21 05:48 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 21 05:48 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 21 05:48 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIzPc2y9acp2"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cD0ZLCK6acp2"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W_uEm4WKacp2",
        "outputId": "5bf3fbc1-5905-45bb-aee1-1564fda6b328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 05:49:37.848537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766296177.868097    9278 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766296177.874359    9278 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766296177.890336    9278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296177.890359    9278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296177.890363    9278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296177.890366    9278 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 05:49:37.894954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-pt/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hmrzd6dAacp2",
        "outputId": "81f061c7-f9d0-43f0-dda5-d741471b073f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 21 05:49 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 05:49 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Dec 21 05:49 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 21 05:49 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 05:49 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 21 05:49 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 21 05:49 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Dec 21 05:49 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 21 05:49 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 21 05:49 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IyyfB3vUacp2",
        "outputId": "994dfab1-a9e5-421a-9d9f-07e119cfc7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp3GEyCcacp2"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "bDrTlmJNacp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jBaXSN4sacp3"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9-VEWZnLacp3"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iLfdpxbwacp3"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "sa9UVWvKacp3",
        "outputId": "555533c5-573e-4318-b407-325eb8073c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl        sharegpt_zh_1K_format.jsonl\n",
            "numina_cot_sharegpt_data_1k.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./data/finetune/numina_cot_sharegpt_data_1k.jsonl ./data/finetune/numina_cot_sharegpt_data_1k.jsonl.bak\n",
        "!cp ./data/finetune/numina_cot_sharegpt_data_1k.strict.jsonl ./data/finetune/numina_cot_sharegpt_data_1k.jsonl\n",
        "!ls -lh ./data/finetune/numina_cot_sharegpt_data_1k.jsonl*"
      ],
      "metadata": {
        "id": "uXz-P0sBd2j8",
        "outputId": "9c723214-4207-4304-e133-333ced1699b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './data/finetune/numina_cot_sharegpt_data_1k.strict.jsonl': No such file or directory\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 05:20 ./data/finetune/numina_cot_sharegpt_data_1k.jsonl.bak\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ihuaxr55acp3",
        "outputId": "92a9ff64-5bbe-4f8e-d300-7669dddbc888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 05:52:40.896955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766296360.939210   10094 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766296360.949872   10094 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766296360.972786   10094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296360.972816   10094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296360.972821   10094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766296360.972824   10094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 05:52:40.981492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-12-21 05:52:46.447\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m348\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Dec21_05-52-46_dd747642cfe0,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.811\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mtrain files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:46.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1meval files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "Generating train split: 2000 examples [00:00, 65478.71 examples/s]\n",
            "Generating validation split: 2000 examples [00:00, 79847.40 examples/s]\n",
            "\u001b[32m2025-12-21 05:52:47.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-21 05:52:47.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m534\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '做广告者。'}, {'from': 'gpt', 'value': '当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。'}, {'from': 'human', 'value': '请为我找到FOMO的替代概念，以便我将其用作广告。\\n例如，我为某个受众利用FOMO来创造类似“我能排除某人吗？”或“我能变得有阶级意识吗？”等概念。\\n您需要为我找到FOMO的其他替代概念，这将对我创建广告内容很有帮助。'}, {'from': 'gpt', 'value': '以下是一些你可以用于广告活动的替代概念：\\n1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\\n2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\\n3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\\n4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\\n5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\\n这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。'}, {'from': 'human', 'value': 'Benissimo，列出其他10个概念。'}, {'from': 'gpt', 'value': '以下是另外10个可供广告活动使用的替代概念：\\n1. 便利性：突出产品或服务可以使人们的生活更简单、更方便。\\n2. 质量：强调产品或服务的质量，并说明它如何与竞争对手相比有所区别。\\n3. 流行度：让购买者感受到他们正在购买受欢迎的物品，有很多其他人已经选择了这个产品。\\n4. 节约：强调产品或服务可以帮助节省金钱或时间。\\n5. 个性化：强调产品或服务可以根据人们的具体需求进行个性化定制。\\n6. 满意保证：强调您的满意保证政策，并说明这可以给购买者带来安心，如果不满意可以退货。\\n7. 支持：强调您在销售前后提供的支持。\\n8. 透明度：强调您的透明度，在报价方面让人们感觉他们确切知道自己正在购买什么。\\n9. 趋势：突显产品或服务与当前趋势保持一致，并说明它如何成为人们生活中的时尚附加品。\\n10. 价值：强调产品或服务的价值，证明购买者可以花费相应的价格获得很多物品。\\n这些都是另外10个替代概念，可供广告活动使用。希望这些可以为你提供新的灵感，让你更好地创建下一个广告内容。'}]}\u001b[0m\n",
            "Running tokenizer on dataset: 100% 1000/1000 [00:12<00:00, 77.98 examples/s]\n",
            "Filter: 100% 998/998 [00:00<00:00, 2990.08 examples/s]\n",
            "\u001b[32m2025-12-21 05:53:02.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m551\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 998\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:02.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:02.724\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m553\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 做广告者。 ASSISTANT:当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。<|endoftext|></s>USER: 请为我找到FOMO的替代概念，以便我将其用作广告。\n",
            "例如，我为某个受众利用FOMO来创造类似“我能排除某人吗？”或“我能变得有阶级意识吗？”等概念。\n",
            "您需要为我找到FOMO的其他替代概念，这将对我创建广告内容很有帮助。 ASSISTANT:以下是一些你可以用于广告活动的替代概念：\n",
            "1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\n",
            "2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\n",
            "3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\n",
            "4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\n",
            "5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\n",
            "这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:02.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>以下是一些你可以用于广告活动的替代概念：\n",
            "1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\n",
            "2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\n",
            "3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\n",
            "4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\n",
            "5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\n",
            "这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:02.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:02.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？'}, {'from': 'gpt', 'value': '男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。'}]}\u001b[0m\n",
            "Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 142.25 examples/s]\n",
            "Filter: 100% 10/10 [00:00<00:00, 1767.59 examples/s]\n",
            "\u001b[32m2025-12-21 05:53:06.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m586\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？ ASSISTANT:男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1m🔧 大模型训练配置:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1m  model_kwargs: {'config': Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            ", 'torch_dtype': torch.bfloat16, 'trust_remote_code': True, 'quantization_config': None, 'low_cpu_mem_usage': True, 'device_map': 'auto'}\u001b[0m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m713\u001b[0m - \u001b[1m✅ 模型加载完成\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[1m📊 模型分布情况:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m718\u001b[0m - \u001b[1m🔧 使用HuggingFace设备映射:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m720\u001b[0m - \u001b[1m  : 0\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m728\u001b[0m - \u001b[1m📈 设备使用统计:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m730\u001b[0m - \u001b[1m  0: 1 个模块\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m751\u001b[0m - \u001b[1m💾 GPU内存使用情况:\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m756\u001b[0m - \u001b[1m  GPU 0: 已分配=0.9GB, 缓存=0.9GB, 总计=14.7GB\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m798\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m813\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m822\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m823\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "\u001b[32m2025-12-21 05:53:06.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m845\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
            "/content/MedicalGPT/supervised_finetuning.py:862: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2025-12-21 05:53:06.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.889\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m876\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ...,  99898,   1773, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,  99898,   1773, 151643],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100]],\n",
            "       device='cuda:0')}\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.923\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m877\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
            "[tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  18137,    103,    101,  32108,\n",
            "         33071,  99180,  35551,  45356,  99180,  35551,  99252,   9370, 104650,\n",
            "        101899, 101895,  99245,  11319,  35560,   3846,   2821,     25,  32664,\n",
            "         99769, 100143,  54542,  24968, 120412,  99180, 101953, 101899, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,    220, 107809,  11622,  25411,\n",
            "         40814, 101454,  24339, 112672, 100625,  28404,  99678,   9370, 114091,\n",
            "        101037,  11319,  35560,   3846,   2821,     25, 103942,  73670,      0,\n",
            "         32181,    247,  20412, 100625,  28404,  99678,  26940,  77419,  38989,\n",
            "         99858,  25067,   9370, 114091,   3837,  59879,  19360,  51461,    229,\n",
            "         40814,  51463,  48443,  73594,  12669,    198,     55,     25,     16,\n",
            "           198,     51,  74045,   7679,   6222,     79,  38940,  39614,    198,\n",
            "            44,     25,     19,     14,     19,    198,     43,     25,     16,\n",
            "            14,     19,    198,     42,  69856,    198,     48,     25,     16,\n",
            "            14,     19,     28,     16,     17,     15,    198,     89,     17,\n",
            "           760,    434,     17,    434,     17,    362,     17,    272,     17,\n",
            "           760,    272,     17,    272,     17,    425,     17,    362,     17,\n",
            "           760,    479,     17,    479,     17,    479,     17,    362,     17,\n",
            "           760,    425,     17,    425,     17,    425,     17,   1147,     17,\n",
            "          9248,     66,      6,     17,    272,      6,     17,    294,      6,\n",
            "            17,    384,      6,     17,    760,    282,      6,     17,    282,\n",
            "             6,     17,    384,      6,     17,    294,      6,     17,    760,\n",
            "           272,      6,     17,    272,      6,     17,    425,     17,    362,\n",
            "            17,    760,    479,     17,    479,     17,    479,     17,   1147,\n",
            "            17,   9248,     89,     17,    760,    272,     17,    272,     17,\n",
            "           294,     17,    384,     17,    760,    282,     17,    282,     17,\n",
            "           384,     17,    294,     17,    760,    272,     17,    272,     17,\n",
            "           425,     17,    362,     17,    760,    479,     17,    479,     17,\n",
            "           479,     17,   1147,     17,   9248,     89,     17,    760,    434,\n",
            "            17,    434,     17,    362,     17,    272,     17,    760,    272,\n",
            "            17,    272,     17,    425,     17,    362,     17,    760,    479,\n",
            "            17,    479,     17,    479,     17,    362,     17,    760,    425,\n",
            "            17,    425,     17,    425,     17,   1147,     17,   9248,     66,\n",
            "             6,     17,    272,      6,     17,    294,      6,     17,    384,\n",
            "             6,     17,    760,    282,      6,     17,    282,      6,     17,\n",
            "           384,     17,    294,     17,    760,    272,      6,     17,    272,\n",
            "             6,     17,    425,     17,    362,     17,    760,    479,     17,\n",
            "           479,     17,    479,     17,   1147,     17,   9248,  13874,  19324,\n",
            "        100137,  40814, 101454,  24339, 116984,  44063, 114091,  31196,  26939,\n",
            "        100646, 103951,  74220,  15946,   3837, 101912,  99350, 101454, 103951,\n",
            "         57191, 106726,  31548,   3837, 105920, 114091,  73670,  18397,  53222,\n",
            "         57191,  23031, 101454,  20742, 100414, 102703,  99898,   1773, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  69372,  98749,  98237,  30534,\n",
            "        106637,  35560,   3846,   2821,     25, 106637, 101158, 101042, 108872,\n",
            "         51575, 105080,   3837, 102215,  99795, 102064,   5373, 104564, 102064,\n",
            "         99998,  20074, 100166,   3837,  71268, 101137, 100414,  72881, 102648,\n",
            "         46448,   1773, 106637, 104820,  20412,  60610,  31196,   9370, 100166,\n",
            "          3837, 102119,  17714,  99794,  68862,  31196,  57191,  99553, 102988,\n",
            "        109963,  66017,   8997, 106637,  66558, 118619,  75768,   3837, 100398,\n",
            "         37029, 104339, 108747,  96555, 106637, 106708, 102064,  33108, 109988,\n",
            "         66017,   1773, 101883, 102716, 106637,  99361, 100630,    510,      9,\n",
            "         61991,  99743, 111228, 106637,     25,    220,  75882,  39907,  45181,\n",
            "         31196,   9370, 102198, 100166,  55286,   3837, 104137, 101042, 103991,\n",
            "        108872,   8997,      9,  61991,  99413, 105798, 106637,     25,    220,\n",
            "         75882,  39907,  45181, 106506, 108872,  55286,   3837, 105798, 101042,\n",
            "        101908,  31196, 100166,   8997,      9,    220, 100520, 100040, 102008,\n",
            "        106637,     25,  32181,    247,  86402,  39907,  37029, 108940, 100520,\n",
            "        100040,  32804,  36407, 101042,  31196,   9370, 100166,   1773, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0')], \n",
            "labels:\n",
            "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,  32664,\n",
            "         99769, 100143,  54542,  24968, 120412,  99180, 101953, 101899, 151643,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100, 103942,  73670,      0,\n",
            "         32181,    247,  20412, 100625,  28404,  99678,  26940,  77419,  38989,\n",
            "         99858,  25067,   9370, 114091,   3837,  59879,  19360,  51461,    229,\n",
            "         40814,  51463,  48443,  73594,  12669,    198,     55,     25,     16,\n",
            "           198,     51,  74045,   7679,   6222,     79,  38940,  39614,    198,\n",
            "            44,     25,     19,     14,     19,    198,     43,     25,     16,\n",
            "            14,     19,    198,     42,  69856,    198,     48,     25,     16,\n",
            "            14,     19,     28,     16,     17,     15,    198,     89,     17,\n",
            "           760,    434,     17,    434,     17,    362,     17,    272,     17,\n",
            "           760,    272,     17,    272,     17,    425,     17,    362,     17,\n",
            "           760,    479,     17,    479,     17,    479,     17,    362,     17,\n",
            "           760,    425,     17,    425,     17,    425,     17,   1147,     17,\n",
            "          9248,     66,      6,     17,    272,      6,     17,    294,      6,\n",
            "            17,    384,      6,     17,    760,    282,      6,     17,    282,\n",
            "             6,     17,    384,      6,     17,    294,      6,     17,    760,\n",
            "           272,      6,     17,    272,      6,     17,    425,     17,    362,\n",
            "            17,    760,    479,     17,    479,     17,    479,     17,   1147,\n",
            "            17,   9248,     89,     17,    760,    272,     17,    272,     17,\n",
            "           294,     17,    384,     17,    760,    282,     17,    282,     17,\n",
            "           384,     17,    294,     17,    760,    272,     17,    272,     17,\n",
            "           425,     17,    362,     17,    760,    479,     17,    479,     17,\n",
            "           479,     17,   1147,     17,   9248,     89,     17,    760,    434,\n",
            "            17,    434,     17,    362,     17,    272,     17,    760,    272,\n",
            "            17,    272,     17,    425,     17,    362,     17,    760,    479,\n",
            "            17,    479,     17,    479,     17,    362,     17,    760,    425,\n",
            "            17,    425,     17,    425,     17,   1147,     17,   9248,     66,\n",
            "             6,     17,    272,      6,     17,    294,      6,     17,    384,\n",
            "             6,     17,    760,    282,      6,     17,    282,      6,     17,\n",
            "           384,     17,    294,     17,    760,    272,      6,     17,    272,\n",
            "             6,     17,    425,     17,    362,     17,    760,    479,     17,\n",
            "           479,     17,    479,     17,   1147,     17,   9248,  13874,  19324,\n",
            "        100137,  40814, 101454,  24339, 116984,  44063, 114091,  31196,  26939,\n",
            "        100646, 103951,  74220,  15946,   3837, 101912,  99350, 101454, 103951,\n",
            "         57191, 106726,  31548,   3837, 105920, 114091,  73670,  18397,  53222,\n",
            "         57191,  23031, 101454,  20742, 100414, 102703,  99898,   1773, 151643],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100, 106637, 101158, 101042, 108872,\n",
            "         51575, 105080,   3837, 102215,  99795, 102064,   5373, 104564, 102064,\n",
            "         99998,  20074, 100166,   3837,  71268, 101137, 100414,  72881, 102648,\n",
            "         46448,   1773, 106637, 104820,  20412,  60610,  31196,   9370, 100166,\n",
            "          3837, 102119,  17714,  99794,  68862,  31196,  57191,  99553, 102988,\n",
            "        109963,  66017,   8997, 106637,  66558, 118619,  75768,   3837, 100398,\n",
            "         37029, 104339, 108747,  96555, 106637, 106708, 102064,  33108, 109988,\n",
            "         66017,   1773, 101883, 102716, 106637,  99361, 100630,    510,      9,\n",
            "         61991,  99743, 111228, 106637,     25,    220,  75882,  39907,  45181,\n",
            "         31196,   9370, 102198, 100166,  55286,   3837, 104137, 101042, 103991,\n",
            "        108872,   8997,      9,  61991,  99413, 105798, 106637,     25,    220,\n",
            "         75882,  39907,  45181, 106506, 108872,  55286,   3837, 105798, 101042,\n",
            "        101908,  31196, 100166,   8997,      9,    220, 100520, 100040, 102008,\n",
            "        106637,     25,  32181,    247,  86402,  39907,  37029, 108940, 100520,\n",
            "        100040,  32804,  36407, 101042,  31196,   9370, 100166,   1773, 151643,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0')]\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:06.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m878\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 骨化性气管支气管病的辅助治疗有些什么？ ASSISTANT:对症支持处理；氩气刀治疗<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-21 05:53:07.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m881\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>对症支持处理；氩气刀治疗<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 151643}.\n",
            "{'loss': 2.5827, 'grad_norm': 1.3944993019104004, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 2.609, 'grad_norm': 1.0852948427200317, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.04}\n",
            "{'loss': 2.7038, 'grad_norm': 1.549014687538147, 'learning_rate': 1.949367088607595e-05, 'epoch': 0.08}\n",
            "{'loss': 2.471, 'grad_norm': 1.0538694858551025, 'learning_rate': 1.8649789029535868e-05, 'epoch': 0.12}\n",
            "{'loss': 2.3965, 'grad_norm': 1.3449335098266602, 'learning_rate': 1.780590717299578e-05, 'epoch': 0.16}\n",
            "{'loss': 2.5249, 'grad_norm': 1.253195881843567, 'learning_rate': 1.6962025316455696e-05, 'epoch': 0.2}\n",
            " 20% 50/250 [02:08<09:32,  2.86s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.66it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 3.152466297149658, 'eval_runtime': 2.0454, 'eval_samples_per_second': 4.889, 'eval_steps_per_second': 1.467, 'epoch': 0.2}\n",
            " 20% 50/250 [02:10<09:32,  2.86s/it]\n",
            "100% 3/3 [00:01<00:00,  1.84it/s]\u001b[A\n",
            "{'loss': 2.5123, 'grad_norm': 1.7407509088516235, 'learning_rate': 1.6118143459915612e-05, 'epoch': 0.24}\n",
            "{'loss': 2.5918, 'grad_norm': 1.2133322954177856, 'learning_rate': 1.5274261603375528e-05, 'epoch': 0.28}\n",
            "{'loss': 2.508, 'grad_norm': 1.4401912689208984, 'learning_rate': 1.4430379746835444e-05, 'epoch': 0.32}\n",
            "{'loss': 2.319, 'grad_norm': 1.1382466554641724, 'learning_rate': 1.358649789029536e-05, 'epoch': 0.36}\n",
            "{'loss': 2.3522, 'grad_norm': 1.7073585987091064, 'learning_rate': 1.2742616033755275e-05, 'epoch': 0.4}\n",
            " 40% 100/250 [04:23<06:58,  2.79s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1033942699432373, 'eval_runtime': 2.0534, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.461, 'epoch': 0.4}\n",
            " 40% 100/250 [04:25<06:58,  2.79s/it]\n",
            "100% 3/3 [00:01<00:00,  1.83it/s]\u001b[A\n",
            "{'loss': 2.283, 'grad_norm': 1.9501125812530518, 'learning_rate': 1.189873417721519e-05, 'epoch': 0.44}\n",
            "{'loss': 2.6048, 'grad_norm': 1.5854929685592651, 'learning_rate': 1.1054852320675107e-05, 'epoch': 0.48}\n",
            "{'loss': 2.272, 'grad_norm': 2.0303213596343994, 'learning_rate': 1.0210970464135021e-05, 'epoch': 0.52}\n",
            "{'loss': 2.2889, 'grad_norm': 2.0720577239990234, 'learning_rate': 9.367088607594937e-06, 'epoch': 0.56}\n",
            "{'loss': 2.2359, 'grad_norm': 1.4198884963989258, 'learning_rate': 8.523206751054853e-06, 'epoch': 0.6}\n",
            " 60% 150/250 [06:33<03:52,  2.32s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.0911412239074707, 'eval_runtime': 2.0582, 'eval_samples_per_second': 4.859, 'eval_steps_per_second': 1.458, 'epoch': 0.6}\n",
            " 60% 150/250 [06:35<03:52,  2.32s/it]\n",
            "100% 3/3 [00:01<00:00,  1.82it/s]\u001b[A\n",
            "{'loss': 2.4231, 'grad_norm': 1.4711718559265137, 'learning_rate': 7.679324894514768e-06, 'epoch': 0.64}\n",
            "{'loss': 2.1647, 'grad_norm': 1.1103184223175049, 'learning_rate': 6.835443037974684e-06, 'epoch': 0.68}\n",
            "{'loss': 2.345, 'grad_norm': 1.1370669603347778, 'learning_rate': 5.9915611814346e-06, 'epoch': 0.72}\n",
            "{'loss': 2.154, 'grad_norm': 1.1209746599197388, 'learning_rate': 5.147679324894516e-06, 'epoch': 0.76}\n",
            "{'loss': 2.3489, 'grad_norm': 1.4529430866241455, 'learning_rate': 4.303797468354431e-06, 'epoch': 0.8}\n",
            " 80% 200/250 [08:50<02:14,  2.69s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.085050106048584, 'eval_runtime': 2.0656, 'eval_samples_per_second': 4.841, 'eval_steps_per_second': 1.452, 'epoch': 0.8}\n",
            " 80% 200/250 [08:52<02:14,  2.69s/it]\n",
            "100% 3/3 [00:01<00:00,  1.83it/s]\u001b[A\n",
            "{'loss': 2.3683, 'grad_norm': 1.7766014337539673, 'learning_rate': 3.459915611814346e-06, 'epoch': 0.84}\n",
            "{'loss': 2.497, 'grad_norm': 1.364638090133667, 'learning_rate': 2.6160337552742622e-06, 'epoch': 0.88}\n",
            "{'loss': 2.6105, 'grad_norm': 2.2798120975494385, 'learning_rate': 1.7721518987341774e-06, 'epoch': 0.92}\n",
            "{'loss': 2.3138, 'grad_norm': 1.303694248199463, 'learning_rate': 9.28270042194093e-07, 'epoch': 0.96}\n",
            "{'loss': 2.4783, 'grad_norm': 1.9348341226577759, 'learning_rate': 8.438818565400844e-08, 'epoch': 1.0}\n",
            "100% 250/250 [11:04<00:00,  2.24s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.64it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.0836503505706787, 'eval_runtime': 2.0606, 'eval_samples_per_second': 4.853, 'eval_steps_per_second': 1.456, 'epoch': 1.0}\n",
            "100% 250/250 [11:06<00:00,  2.24s/it]\n",
            "100% 3/3 [00:01<00:00,  1.82it/s]\u001b[A\n",
            "{'train_runtime': 666.6818, 'train_samples_per_second': 1.497, 'train_steps_per_second': 0.375, 'train_loss': 2.4149569931030275, 'epoch': 1.0}\n",
            "100% 250/250 [11:06<00:00,  2.67s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   878597GF\n",
            "  train_loss               =      2.415\n",
            "  train_runtime            = 0:11:06.68\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      1.497\n",
            "  train_steps_per_second   =      0.375\n",
            "\u001b[32m2025-12-21 06:04:14.108\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m898\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 666.6818, 'train_samples_per_second': 1.497, 'train_steps_per_second': 0.375, 'total_flos': 943387169931264.0, 'train_loss': 2.4149569931030275, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
            "\u001b[32m2025-12-21 06:04:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m899\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
            "\u001b[32m2025-12-21 06:04:14.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m908\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 3/3 [00:01<00:00,  1.69it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =     3.0833\n",
            "  eval_runtime            = 0:00:02.01\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      4.968\n",
            "  eval_steps_per_second   =       1.49\n",
            "  perplexity              =    21.8307\n",
            "\u001b[32m2025-12-21 06:04:16.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m921\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.0833163261413574, 'eval_runtime': 2.0127, 'eval_samples_per_second': 4.968, 'eval_steps_per_second': 1.49, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 21.830680136895566}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pzWlmzFQacp3",
        "outputId": "eb1a253f-1f4a-4484-82ab-ce5f8aabdf02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Dec 21 06:04 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 21 06:04 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 21 06:04 added_tokens.json\n",
            "-rw-r--r-- 1 root root  429 Dec 21 06:04 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 06:04 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 06:04 \u001b[0m\u001b[01;34mcheckpoint-250\u001b[0m/\n",
            "-rw-r--r-- 1 root root  220 Dec 21 06:04 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 06:04 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Dec 21 06:04 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 21 05:53 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Dec 21 06:04 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.7K Dec 21 06:04 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 6.0K Dec 21 06:04 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 21 06:04 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 21 06:04 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lS3u_MXvacp3"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mT_Shpwoacp3"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rtZ3fd2hacp3",
        "outputId": "f89b1c01-a9d6-4e64-c3eb-37e2aaffa2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 06:05:54.820829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766297154.840181   13494 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766297154.846079   13494 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766297154.861332   13494 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297154.861356   13494 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297154.861360   13494 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297154.861363   13494 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 06:05:54.866983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-pt\n",
            "LoRA model: outputs-sft-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The tokenizer you are loading from 'merged-pt' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to ./merged-sft\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0RXZ9D5eacp3",
        "outputId": "8a9daabd-6312-4c70-91f4-3047ecec8bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 21 06:06 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 06:06 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Dec 21 06:06 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 21 06:06 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 06:06 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 21 06:06 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 21 06:06 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Dec 21 06:06 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 21 06:06 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 21 06:06 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8wqdhiT4acp3",
        "outputId": "2156da23-d3ba-4909-e201-748330d639e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2Yh2T31Racp3"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "wwinXnVcacp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "82zJXmU8acp3"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dQwcgAsBacp3"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_GpeHFCXacp3"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9rYj7UZhacp3",
        "outputId": "033e5b25-35f9-4552-9daf-15bb93abe685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OaMafoViacp4",
        "outputId": "d2c6cc72-88a7-4a2e-b306-9b6ee7515ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 06:08:08.110886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766297288.130376   14141 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766297288.136314   14141 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766297288.152548   14141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297288.152573   14141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297288.152579   14141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766297288.152582   14141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 06:08:08.157612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-12-21 06:08:14.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mParse args: ScriptArguments(model_name_or_path='./merged-sft', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', template_name='qwen', per_device_train_batch_size=3, per_device_eval_batch_size=1, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=500, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, peft_path=None, do_train=True, do_eval=True, learning_rate=0.0005, lr_scheduler_type='cosine', warmup_steps=100, weight_decay=0.05, optim='adamw_torch', fp16=False, bf16=True, gradient_checkpointing=True, gradient_accumulation_steps=4, save_steps=50, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-v1', max_steps=100, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:14.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:14.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='./merged-sft', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:14.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:14.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m321\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "Running tokenizer on dataset (num_proc=4): 100% 500/500 [00:00<00:00, 1660.41 examples/s]\n",
            "Filter: 100% 500/500 [00:00<00:00, 25840.36 examples/s]\n",
            "\u001b[32m2025-12-21 06:08:15.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m334\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 160\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m335\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m337\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "你是一个公正、不加审查、有帮助的助手。\n",
            "<|im_start|>user\n",
            "将一根灰发拔掉或剪掉会导致多根灰发长回来吗？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m338\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "不，拔掉或剪掉一根白发不会导致多根白发长出来。每个毛囊只能长出一根头发，所以只会重新长出一根头发。这种误解可能源于随着时间的推移灰发逐渐增多，人们错误地将剪掉或拔掉灰发与增多联系起来。\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "不，拔掉或剪掉一根白发不会导致多根白发长出来。白发被认为是由于毛囊黑色素产生的变化而产生的，这是随着年龄增长自然发生的。当一根白发被拔掉时，底下的毛囊仍然不受影响，继续像往常一样生长头发。\n",
            "\n",
            "然而，重要的是要注意，压力、疾病和某些药物可能会加速头发变白的过程。此外，有些人在拔掉或剪掉大量头发后可能会暂时脱发，但这与新白发的生长是不同的。\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m351\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 160\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "你是一个非常聪明的AI助手，非常擅长按照指示行事。尽你所能地帮助。\n",
            "<|im_start|>user\n",
            "期末考试 问题1. 罗伯特·P·凯利曾任CEO的公司是在哪一年成立的？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "为了帮助您解答这个问题，我需要知道罗伯特·P·凯利曾担任过的公司的名称。请提供公司名称。\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "当然！罗伯特·P·凯利曾担任首席执行官的公司Uber成立于2009年3月28日。\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:15.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mDevice map: auto\u001b[0m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[32m2025-12-21 06:08:16.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:16.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m449\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "Extracting prompt in train dataset: 100% 160/160 [00:00<00:00, 4309.39 examples/s]\n",
            "Applying chat template to train dataset: 100% 160/160 [00:00<00:00, 9450.22 examples/s]\n",
            "Tokenizing train dataset: 100% 160/160 [00:01<00:00, 104.55 examples/s]\n",
            "Extracting prompt in eval dataset: 100% 160/160 [00:00<00:00, 6346.35 examples/s]\n",
            "Applying chat template to eval dataset: 100% 160/160 [00:00<00:00, 9608.67 examples/s]\n",
            "Tokenizing eval dataset: 100% 160/160 [00:01<00:00, 96.56 examples/s] \n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2025-12-21 06:08:30.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprint_trainable_parameters\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mtrainable params: 4399104 || all params: 498431872 || trainable%: 0.8825888244963597\u001b[0m\n",
            "\u001b[32m2025-12-21 06:08:30.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 151643}.\n",
            "  0% 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 0.6931, 'grad_norm': 6.290706634521484, 'learning_rate': 0.0, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -206.6688232421875, 'logps/rejected': -205.71841430664062, 'logits/chosen': -0.6451120376586914, 'logits/rejected': -0.6582886576652527, 'epoch': 0.07}\n",
            "{'loss': 0.6931, 'grad_norm': 6.4870758056640625, 'learning_rate': 5e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -192.10769653320312, 'logps/rejected': -167.12786865234375, 'logits/chosen': -1.0062906742095947, 'logits/rejected': -1.0843729972839355, 'epoch': 0.15}\n",
            "{'loss': 0.6888, 'grad_norm': 6.8669819831848145, 'learning_rate': 1e-05, 'rewards/chosen': 0.01884145848453045, 'rewards/rejected': 0.00960866641253233, 'rewards/accuracies': 0.5, 'rewards/margins': 0.009232791140675545, 'logps/chosen': -230.5164794921875, 'logps/rejected': -230.57850646972656, 'logits/chosen': -0.6426860094070435, 'logits/rejected': -0.7407822012901306, 'epoch': 0.22}\n",
            "{'loss': 0.6964, 'grad_norm': 6.644769191741943, 'learning_rate': 1.5e-05, 'rewards/chosen': 0.01112252939492464, 'rewards/rejected': 0.0164986290037632, 'rewards/accuracies': 0.5, 'rewards/margins': -0.00537610100582242, 'logps/chosen': -207.7842559814453, 'logps/rejected': -180.0309295654297, 'logits/chosen': -0.7974905371665955, 'logits/rejected': -0.7285467982292175, 'epoch': 0.3}\n",
            "{'loss': 0.691, 'grad_norm': 7.351845741271973, 'learning_rate': 2e-05, 'rewards/chosen': 0.012109057977795601, 'rewards/rejected': 0.006021562963724136, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0060874950140714645, 'logps/chosen': -223.90423583984375, 'logps/rejected': -191.15090942382812, 'logits/chosen': -1.0173161029815674, 'logits/rejected': -0.9452279806137085, 'epoch': 0.37}\n",
            "{'loss': 0.7075, 'grad_norm': 7.533649921417236, 'learning_rate': 2.5e-05, 'rewards/chosen': -0.0010578311048448086, 'rewards/rejected': 0.02649485319852829, 'rewards/accuracies': 0.4166666865348816, 'rewards/margins': -0.027552684769034386, 'logps/chosen': -201.8058624267578, 'logps/rejected': -245.48471069335938, 'logits/chosen': -1.0069091320037842, 'logits/rejected': -1.1008248329162598, 'epoch': 0.44}\n",
            "{'loss': 0.6899, 'grad_norm': 5.954451560974121, 'learning_rate': 3e-05, 'rewards/chosen': 0.0260667335242033, 'rewards/rejected': 0.01917244680225849, 'rewards/accuracies': 0.5833333730697632, 'rewards/margins': 0.00689428485929966, 'logps/chosen': -182.2082977294922, 'logps/rejected': -73.42024230957031, 'logits/chosen': -0.6894140243530273, 'logits/rejected': -0.7790992259979248, 'epoch': 0.52}\n",
            "{'loss': 0.6877, 'grad_norm': 6.776644229888916, 'learning_rate': 3.5000000000000004e-05, 'rewards/chosen': 0.023206554353237152, 'rewards/rejected': 0.010854626074433327, 'rewards/accuracies': 0.5833333134651184, 'rewards/margins': 0.012351928278803825, 'logps/chosen': -242.23837280273438, 'logps/rejected': -199.10910034179688, 'logits/chosen': -0.653409481048584, 'logits/rejected': -0.5416505932807922, 'epoch': 0.59}\n",
            "{'loss': 0.7059, 'grad_norm': 6.0263519287109375, 'learning_rate': 4e-05, 'rewards/chosen': 0.01601340062916279, 'rewards/rejected': 0.03940029442310333, 'rewards/accuracies': 0.25, 'rewards/margins': -0.023386893793940544, 'logps/chosen': -217.0595245361328, 'logps/rejected': -154.1162567138672, 'logits/chosen': -0.7723674774169922, 'logits/rejected': -0.869279146194458, 'epoch': 0.67}\n",
            "{'loss': 0.6951, 'grad_norm': 6.091519355773926, 'learning_rate': 4.4999999999999996e-05, 'rewards/chosen': 0.024469422176480293, 'rewards/rejected': 0.027851425111293793, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0033820010721683502, 'logps/chosen': -179.2241973876953, 'logps/rejected': -155.55484008789062, 'logits/chosen': -0.7765111923217773, 'logits/rejected': -0.8205541372299194, 'epoch': 0.74}\n",
            " 10% 10/100 [01:43<15:35, 10.39s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:43,  3.66it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:59,  2.64it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:55,  2.80it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:05,  2.38it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.34it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.26it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:05,  2.29it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:09,  2.15it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:02,  2.37it/s]\u001b[A\n",
            "  8% 12/160 [00:04<00:59,  2.47it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.33it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:07,  2.17it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:07,  2.14it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.28it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.48it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.27it/s]\u001b[A\n",
            " 12% 19/160 [00:07<00:57,  2.47it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.64it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.67it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:55,  2.47it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.74it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:49,  2.75it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:48,  2.76it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:58,  2.28it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:59,  2.21it/s]\u001b[A\n",
            " 18% 29/160 [00:11<00:53,  2.45it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:52,  2.49it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:58,  2.22it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:52,  2.43it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.62it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:46,  2.72it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.73it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.96it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.88it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.92it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.56it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.59it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.32it/s]\u001b[A\n",
            " 28% 44/160 [00:17<00:54,  2.15it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:49,  2.30it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.15it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.11it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:46,  2.41it/s]\u001b[A\n",
            " 31% 49/160 [00:19<00:44,  2.48it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:42,  2.56it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.47it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:47,  2.26it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.20it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:43,  2.41it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:43,  2.39it/s]\u001b[A\n",
            " 35% 56/160 [00:22<00:43,  2.41it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:37,  2.74it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:37,  2.66it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:38,  2.61it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:41,  2.38it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:43,  2.28it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.39it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.20it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.41it/s]\u001b[A\n",
            " 41% 66/160 [00:26<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.34it/s]\u001b[A\n",
            " 42% 68/160 [00:27<00:41,  2.23it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.57it/s]\u001b[A\n",
            " 44% 71/160 [00:28<00:32,  2.73it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:36,  2.39it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.21it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:36,  2.33it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.71it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:33,  2.48it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:36,  2.25it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.20it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.50it/s]\u001b[A\n",
            " 50% 80/160 [00:32<00:32,  2.45it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.42it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:34,  2.23it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.35it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.33it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.21it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.32it/s]\u001b[A\n",
            " 54% 87/160 [00:35<00:31,  2.29it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:29,  2.46it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:28,  2.50it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.67it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.89it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.49it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.74it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.85it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.44it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.61it/s]\u001b[A\n",
            " 61% 98/160 [00:39<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:40<00:24,  2.49it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.43it/s]\u001b[A\n",
            " 64% 102/160 [00:41<00:27,  2.13it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.19it/s]\u001b[A\n",
            " 65% 104/160 [00:42<00:26,  2.08it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.07it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.30it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.41it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.23it/s]\u001b[A\n",
            " 68% 109/160 [00:44<00:22,  2.26it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.04it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.98it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.86it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.00it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.93it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.16it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:19,  2.28it/s]\u001b[A\n",
            " 73% 117/160 [00:48<00:18,  2.29it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.20it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.41it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.21it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 122/160 [00:50<00:15,  2.44it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.31it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:13,  2.51it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.44it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.67it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.17it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.20it/s]\u001b[A\n",
            " 84% 134/160 [00:55<00:10,  2.49it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.66it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.34it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.42it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.59it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.43it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.45it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.28it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.51it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.54it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.48it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.33it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.44it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.24it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.29it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.40it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.05it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.04it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.04it/s]\u001b[A\n",
            " 99% 159/160 [01:06<00:00,  2.11it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6468344926834106, 'eval_runtime': 67.6867, 'eval_samples_per_second': 2.364, 'eval_steps_per_second': 2.364, 'eval_rewards/chosen': 0.09400160610675812, 'eval_rewards/rejected': -0.004978679586201906, 'eval_rewards/accuracies': 0.8374999761581421, 'eval_rewards/margins': 0.09898027777671814, 'eval_logps/chosen': -211.6715545654297, 'eval_logps/rejected': -185.10574340820312, 'eval_logits/chosen': -0.7737936973571777, 'eval_logits/rejected': -0.7893381118774414, 'epoch': 0.74}\n",
            " 10% 10/100 [02:51<15:35, 10.39s/it]\n",
            "100% 160/160 [01:07<00:00,  2.18it/s]\u001b[A\n",
            "{'loss': 0.6774, 'grad_norm': 6.476687908172607, 'learning_rate': 5e-05, 'rewards/chosen': 0.039616331458091736, 'rewards/rejected': 0.006150689907371998, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.033465638756752014, 'logps/chosen': -281.73150634765625, 'logps/rejected': -237.26284790039062, 'logits/chosen': -0.7830686569213867, 'logits/rejected': -0.813717246055603, 'epoch': 0.81}\n",
            "{'loss': 0.6918, 'grad_norm': 6.344503402709961, 'learning_rate': 5.5e-05, 'rewards/chosen': 0.06895262748003006, 'rewards/rejected': 0.06374981999397278, 'rewards/accuracies': 0.5833333134651184, 'rewards/margins': 0.005202798172831535, 'logps/chosen': -229.14169311523438, 'logps/rejected': -193.61380004882812, 'logits/chosen': -0.7524988055229187, 'logits/rejected': -0.6790183186531067, 'epoch': 0.89}\n",
            "{'loss': 0.6759, 'grad_norm': 6.667569637298584, 'learning_rate': 6e-05, 'rewards/chosen': 0.09480103105306625, 'rewards/rejected': 0.057742487639188766, 'rewards/accuracies': 0.7500000596046448, 'rewards/margins': 0.03705854341387749, 'logps/chosen': -197.2861328125, 'logps/rejected': -159.6384735107422, 'logits/chosen': -0.8046575784683228, 'logits/rejected': -0.8407187461853027, 'epoch': 0.96}\n",
            "{'loss': 0.7391, 'grad_norm': 12.399211883544922, 'learning_rate': 6.500000000000001e-05, 'rewards/chosen': 0.016139984130859375, 'rewards/rejected': 0.1123432144522667, 'rewards/accuracies': 0.0, 'rewards/margins': -0.09620323032140732, 'logps/chosen': -152.88250732421875, 'logps/rejected': -122.52511596679688, 'logits/chosen': -1.2872450351715088, 'logits/rejected': -1.4102904796600342, 'epoch': 1.0}\n",
            "{'loss': 0.5365, 'grad_norm': 5.6636834144592285, 'learning_rate': 7.000000000000001e-05, 'rewards/chosen': 0.42634862661361694, 'rewards/rejected': 0.07322235405445099, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.35312628746032715, 'logps/chosen': -277.2364501953125, 'logps/rejected': -209.11572265625, 'logits/chosen': -0.864112377166748, 'logits/rejected': -0.8099696040153503, 'epoch': 1.07}\n",
            "{'loss': 0.5684, 'grad_norm': 5.1209235191345215, 'learning_rate': 7.5e-05, 'rewards/chosen': 0.32504215836524963, 'rewards/rejected': 0.017435841262340546, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.3076063096523285, 'logps/chosen': -160.42210388183594, 'logps/rejected': -182.24366760253906, 'logits/chosen': -0.7682992219924927, 'logits/rejected': -0.8088275194168091, 'epoch': 1.15}\n",
            "{'loss': 0.5154, 'grad_norm': 4.80903434753418, 'learning_rate': 8e-05, 'rewards/chosen': 0.4264112412929535, 'rewards/rejected': 0.003592364490032196, 'rewards/accuracies': 1.0, 'rewards/margins': 0.4228188693523407, 'logps/chosen': -205.51205444335938, 'logps/rejected': -176.0706329345703, 'logits/chosen': -0.8299562931060791, 'logits/rejected': -0.7706652879714966, 'epoch': 1.22}\n",
            "{'loss': 0.5533, 'grad_norm': 4.442784309387207, 'learning_rate': 8.5e-05, 'rewards/chosen': 0.24672357738018036, 'rewards/rejected': -0.07477786391973495, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.3215014338493347, 'logps/chosen': -163.08729553222656, 'logps/rejected': -117.20025634765625, 'logits/chosen': -0.7017644047737122, 'logits/rejected': -0.8164618015289307, 'epoch': 1.3}\n",
            "{'loss': 0.4725, 'grad_norm': 4.423763751983643, 'learning_rate': 8.999999999999999e-05, 'rewards/chosen': 0.5012944936752319, 'rewards/rejected': -0.077513188123703, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.5788077116012573, 'logps/chosen': -222.77040100097656, 'logps/rejected': -206.31039428710938, 'logits/chosen': -1.0518059730529785, 'logits/rejected': -0.7492954730987549, 'epoch': 1.37}\n",
            "{'loss': 0.4508, 'grad_norm': 4.649587154388428, 'learning_rate': 9.5e-05, 'rewards/chosen': 0.5387580990791321, 'rewards/rejected': -0.07368577271699905, 'rewards/accuracies': 1.0, 'rewards/margins': 0.6124438047409058, 'logps/chosen': -179.60955810546875, 'logps/rejected': -157.79811096191406, 'logits/chosen': -0.9205499291419983, 'logits/rejected': -1.043377161026001, 'epoch': 1.44}\n",
            " 20% 20/100 [04:27<14:29, 10.86s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.81it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.78it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:55,  2.82it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:06,  2.34it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:03,  2.41it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:07,  2.25it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:09,  2.20it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:11,  2.10it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.33it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.44it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.12it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.27it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.48it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.27it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.47it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:52,  2.64it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.67it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.45it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.72it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.74it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.27it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.26it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.54it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:49,  2.60it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:55,  2.32it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.46it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:49,  2.59it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:46,  2.68it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:47,  2.64it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:44,  2.77it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:44,  2.75it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:44,  2.73it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:42,  2.84it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.64it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.14it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.29it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.13it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.10it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:46,  2.39it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.46it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.55it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.46it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.25it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.20it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.40it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:40,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.86it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.83it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.81it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.53it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.34it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.40it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:44,  2.17it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:40,  2.35it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.55it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.31it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.17it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.71it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.37it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.20it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:36,  2.32it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.70it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.47it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:36,  2.24it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.19it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.49it/s]\u001b[A\n",
            " 50% 80/160 [00:32<00:32,  2.44it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.42it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:35,  2.23it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.36it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.35it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.25it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.38it/s]\u001b[A\n",
            " 54% 87/160 [00:35<00:30,  2.36it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.56it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.61it/s]\u001b[A\n",
            " 56% 90/160 [00:36<00:25,  2.71it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.85it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.44it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:25,  2.65it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:24,  2.73it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:25,  2.58it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.41it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.60it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.71it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.72it/s]\u001b[A\n",
            " 62% 100/160 [00:40<00:24,  2.47it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:41<00:27,  2.12it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.18it/s]\u001b[A\n",
            " 65% 104/160 [00:42<00:27,  2.07it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.07it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.30it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.42it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.22it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.25it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.04it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.97it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.87it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.05it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.98it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.24it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.37it/s]\u001b[A\n",
            " 73% 117/160 [00:48<00:18,  2.32it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.21it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:17,  2.39it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.15it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.20it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.42it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.28it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:15,  2.39it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.49it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.34it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.44it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.52it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.69it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.39it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.38it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.21it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.50it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.67it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.35it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.42it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.60it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.52it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.57it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.40it/s]\u001b[A\n",
            " 90% 144/160 [00:59<00:06,  2.66it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.68it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.52it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.34it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:05,  2.39it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.33it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.08it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.23it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.08it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.28it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.40it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.04it/s]\u001b[A\n",
            " 98% 157/160 [01:05<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.05it/s]\u001b[A\n",
            " 99% 159/160 [01:06<00:00,  2.12it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.38993048667907715, 'eval_runtime': 67.6314, 'eval_samples_per_second': 2.366, 'eval_steps_per_second': 2.366, 'eval_rewards/chosen': 0.6456032991409302, 'eval_rewards/rejected': -0.1880715787410736, 'eval_rewards/accuracies': 0.981249988079071, 'eval_rewards/margins': 0.8336749076843262, 'eval_logps/chosen': -206.155517578125, 'eval_logps/rejected': -186.93666076660156, 'eval_logits/chosen': -0.7886000871658325, 'eval_logits/rejected': -0.8029583692550659, 'epoch': 1.44}\n",
            " 20% 20/100 [05:35<14:29, 10.86s/it]\n",
            "100% 160/160 [01:07<00:00,  2.19it/s]\u001b[A\n",
            "{'loss': 0.4486, 'grad_norm': 4.1712470054626465, 'learning_rate': 0.0001, 'rewards/chosen': 0.517262876033783, 'rewards/rejected': -0.1383519023656845, 'rewards/accuracies': 1.0, 'rewards/margins': 0.6556147933006287, 'logps/chosen': -209.43162536621094, 'logps/rejected': -209.99343872070312, 'logits/chosen': -0.6807923913002014, 'logits/rejected': -0.5779313445091248, 'epoch': 1.52}\n",
            "{'loss': 0.4094, 'grad_norm': 4.707624435424805, 'learning_rate': 0.000105, 'rewards/chosen': 0.5062459707260132, 'rewards/rejected': -0.20317108929157257, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7094171047210693, 'logps/chosen': -183.24305725097656, 'logps/rejected': -223.48866271972656, 'logits/chosen': -0.5256881713867188, 'logits/rejected': -0.6591441035270691, 'epoch': 1.59}\n",
            "{'loss': 0.4245, 'grad_norm': 4.545163631439209, 'learning_rate': 0.00011, 'rewards/chosen': 0.5381637811660767, 'rewards/rejected': -0.1700664609670639, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7082302570343018, 'logps/chosen': -222.921875, 'logps/rejected': -165.7816162109375, 'logits/chosen': -0.8463091850280762, 'logits/rejected': -0.9667964577674866, 'epoch': 1.67}\n",
            "{'loss': 0.4023, 'grad_norm': 4.6946024894714355, 'learning_rate': 0.000115, 'rewards/chosen': 0.5686571598052979, 'rewards/rejected': -0.22948476672172546, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7981418967247009, 'logps/chosen': -166.86026000976562, 'logps/rejected': -189.34381103515625, 'logits/chosen': -0.8437769412994385, 'logits/rejected': -0.8315625190734863, 'epoch': 1.74}\n",
            "{'loss': 0.3668, 'grad_norm': 3.764993667602539, 'learning_rate': 0.00012, 'rewards/chosen': 0.6273578405380249, 'rewards/rejected': -0.3814353942871094, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 1.0087933540344238, 'logps/chosen': -225.02549743652344, 'logps/rejected': -195.58267211914062, 'logits/chosen': -0.8162354230880737, 'logits/rejected': -0.8036215305328369, 'epoch': 1.81}\n",
            "{'loss': 0.3368, 'grad_norm': 3.748976469039917, 'learning_rate': 0.000125, 'rewards/chosen': 0.6574515104293823, 'rewards/rejected': -0.46341270208358765, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 1.1208641529083252, 'logps/chosen': -204.30215454101562, 'logps/rejected': -195.77188110351562, 'logits/chosen': -0.672493577003479, 'logits/rejected': -0.9518102407455444, 'epoch': 1.89}\n",
            "{'loss': 0.2141, 'grad_norm': 2.6503145694732666, 'learning_rate': 0.00013000000000000002, 'rewards/chosen': 1.137885332107544, 'rewards/rejected': -0.5011044144630432, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6389896869659424, 'logps/chosen': -256.7754821777344, 'logps/rejected': -219.6432647705078, 'logits/chosen': -0.771245002746582, 'logits/rejected': -0.7747360467910767, 'epoch': 1.96}\n",
            "{'loss': 0.4823, 'grad_norm': 11.219440460205078, 'learning_rate': 0.000135, 'rewards/chosen': 0.6342911124229431, 'rewards/rejected': -0.23500919342041016, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8693003058433533, 'logps/chosen': -288.9604187011719, 'logps/rejected': -117.55281066894531, 'logits/chosen': -1.0868419408798218, 'logits/rejected': -0.8690600991249084, 'epoch': 2.0}\n",
            "{'loss': 0.1355, 'grad_norm': 1.8952391147613525, 'learning_rate': 0.00014000000000000001, 'rewards/chosen': 1.5430656671524048, 'rewards/rejected': -0.8580570220947266, 'rewards/accuracies': 1.0, 'rewards/margins': 2.401122570037842, 'logps/chosen': -271.97454833984375, 'logps/rejected': -200.99118041992188, 'logits/chosen': -0.8238396644592285, 'logits/rejected': -0.7063858509063721, 'epoch': 2.07}\n",
            "{'loss': 0.1498, 'grad_norm': 2.477445125579834, 'learning_rate': 0.000145, 'rewards/chosen': 1.1521823406219482, 'rewards/rejected': -1.143908977508545, 'rewards/accuracies': 1.0, 'rewards/margins': 2.296091318130493, 'logps/chosen': -219.09701538085938, 'logps/rejected': -245.89697265625, 'logits/chosen': -0.9364245533943176, 'logits/rejected': -0.7617424726486206, 'epoch': 2.15}\n",
            " 30% 30/100 [07:14<12:22, 10.60s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:43,  3.64it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:59,  2.62it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:57,  2.72it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:07,  2.29it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:03,  2.42it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:06,  2.29it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.22it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.32it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:07,  2.15it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.12it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.27it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.48it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.27it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.47it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.64it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:51,  2.67it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:55,  2.47it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.74it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:49,  2.75it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.75it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:58,  2.28it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.27it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.50it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:52,  2.49it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:57,  2.23it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:53,  2.39it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:51,  2.49it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:47,  2.64it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:46,  2.67it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.90it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:43,  2.85it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.80it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.90it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.56it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.60it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.32it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.15it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.28it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.13it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.10it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.38it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.46it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.54it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.47it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:47,  2.26it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:43,  2.41it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.81it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.74it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:37,  2.69it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:40,  2.42it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:43,  2.26it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.37it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.19it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.40it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.59it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.23it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.57it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.73it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.38it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.20it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.32it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.69it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.46it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:36,  2.25it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.20it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.51it/s]\u001b[A\n",
            " 50% 80/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.42it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:34,  2.23it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.36it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.26it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.37it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.31it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:29,  2.48it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:28,  2.53it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.64it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.80it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.44it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.69it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.83it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:24,  2.64it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.45it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:23,  2.63it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.78it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.77it/s]\u001b[A\n",
            " 62% 100/160 [00:40<00:23,  2.50it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.44it/s]\u001b[A\n",
            " 64% 102/160 [00:41<00:27,  2.13it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.18it/s]\u001b[A\n",
            " 65% 104/160 [00:42<00:26,  2.08it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.06it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.29it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.41it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.23it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.27it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.05it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.98it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.86it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.05it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.95it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.17it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:19,  2.28it/s]\u001b[A\n",
            " 73% 117/160 [00:48<00:19,  2.26it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.16it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:17,  2.38it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.19it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.22it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.43it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.30it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:15,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.48it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.33it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.52it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.66it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.16it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.20it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.48it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.66it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.35it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.41it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.59it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.51it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.54it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.33it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.54it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.57it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.44it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.31it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.43it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.26it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.17it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.31it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.42it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.05it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.12it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.12922365963459015, 'eval_runtime': 67.736, 'eval_samples_per_second': 2.362, 'eval_steps_per_second': 2.362, 'eval_rewards/chosen': 1.3321880102157593, 'eval_rewards/rejected': -1.194435477256775, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 2.5266237258911133, 'eval_logps/chosen': -199.28968811035156, 'eval_logps/rejected': -197.00030517578125, 'eval_logits/chosen': -0.7758661508560181, 'eval_logits/rejected': -0.7756366729736328, 'epoch': 2.15}\n",
            " 30% 30/100 [08:22<12:22, 10.60s/it]\n",
            "100% 160/160 [01:07<00:00,  2.16it/s]\u001b[A\n",
            "{'loss': 0.0847, 'grad_norm': 1.5257899761199951, 'learning_rate': 0.00015, 'rewards/chosen': 1.2875646352767944, 'rewards/rejected': -1.420185923576355, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7077505588531494, 'logps/chosen': -243.7599334716797, 'logps/rejected': -223.4807891845703, 'logits/chosen': -0.874351441860199, 'logits/rejected': -0.817105770111084, 'epoch': 2.22}\n",
            "{'loss': 0.0641, 'grad_norm': 1.2650854587554932, 'learning_rate': 0.000155, 'rewards/chosen': 1.4611951112747192, 'rewards/rejected': -2.2205874919891357, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6817824840545654, 'logps/chosen': -241.41177368164062, 'logps/rejected': -337.3201904296875, 'logits/chosen': -0.7723499536514282, 'logits/rejected': -0.9980215430259705, 'epoch': 2.3}\n",
            "{'loss': 0.1037, 'grad_norm': 1.6826666593551636, 'learning_rate': 0.00016, 'rewards/chosen': 1.1136903762817383, 'rewards/rejected': -1.9930875301361084, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1067779064178467, 'logps/chosen': -149.97021484375, 'logps/rejected': -191.77122497558594, 'logits/chosen': -0.8853858709335327, 'logits/rejected': -0.7467656135559082, 'epoch': 2.37}\n",
            "{'loss': 0.0912, 'grad_norm': 1.775267481803894, 'learning_rate': 0.000165, 'rewards/chosen': 1.2034159898757935, 'rewards/rejected': -2.0636112689971924, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2670273780822754, 'logps/chosen': -205.33779907226562, 'logps/rejected': -244.68511962890625, 'logits/chosen': -0.6448060870170593, 'logits/rejected': -0.5702686309814453, 'epoch': 2.44}\n",
            "{'loss': 0.0877, 'grad_norm': 1.2489086389541626, 'learning_rate': 0.00017, 'rewards/chosen': 1.461140751838684, 'rewards/rejected': -1.4787025451660156, 'rewards/accuracies': 1.0, 'rewards/margins': 2.93984317779541, 'logps/chosen': -201.9982147216797, 'logps/rejected': -182.8516845703125, 'logits/chosen': -0.6447349786758423, 'logits/rejected': -0.6970506906509399, 'epoch': 2.52}\n",
            "{'loss': 0.0864, 'grad_norm': 2.529794216156006, 'learning_rate': 0.000175, 'rewards/chosen': 1.546238660812378, 'rewards/rejected': -1.4772392511367798, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0234780311584473, 'logps/chosen': -180.06410217285156, 'logps/rejected': -159.76589965820312, 'logits/chosen': -0.7386287450790405, 'logits/rejected': -0.6917625069618225, 'epoch': 2.59}\n",
            "{'loss': 0.1036, 'grad_norm': 1.770241618156433, 'learning_rate': 0.00017999999999999998, 'rewards/chosen': 0.7574775815010071, 'rewards/rejected': -2.335632562637329, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0931100845336914, 'logps/chosen': -156.37844848632812, 'logps/rejected': -180.91726684570312, 'logits/chosen': -0.8078675270080566, 'logits/rejected': -0.7229470610618591, 'epoch': 2.67}\n",
            "{'loss': 0.1117, 'grad_norm': 3.066399097442627, 'learning_rate': 0.000185, 'rewards/chosen': 1.2859340906143188, 'rewards/rejected': -2.153210163116455, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4391441345214844, 'logps/chosen': -196.35702514648438, 'logps/rejected': -161.83265686035156, 'logits/chosen': -0.9490337371826172, 'logits/rejected': -1.1754788160324097, 'epoch': 2.74}\n",
            "{'loss': 0.1458, 'grad_norm': 1.9745452404022217, 'learning_rate': 0.00019, 'rewards/chosen': 1.3339678049087524, 'rewards/rejected': -1.5915921926498413, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9255597591400146, 'logps/chosen': -179.8960418701172, 'logps/rejected': -133.01583862304688, 'logits/chosen': -0.8811440467834473, 'logits/rejected': -1.005414605140686, 'epoch': 2.81}\n",
            "{'loss': 0.0863, 'grad_norm': 1.5067648887634277, 'learning_rate': 0.00019500000000000002, 'rewards/chosen': 1.0267760753631592, 'rewards/rejected': -2.496661424636841, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5234375, 'logps/chosen': -178.84957885742188, 'logps/rejected': -178.55320739746094, 'logits/chosen': -0.8952406644821167, 'logits/rejected': -0.8747154474258423, 'epoch': 2.89}\n",
            " 40% 40/100 [10:04<10:40, 10.67s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.77it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.76it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.89it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.41it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.34it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.25it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.35it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.32it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:07,  2.15it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.12it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.26it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.47it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.27it/s]\u001b[A\n",
            " 12% 19/160 [00:07<00:57,  2.45it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:54,  2.56it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:54,  2.56it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:58,  2.36it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:53,  2.58it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:51,  2.62it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:50,  2.67it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.23it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.24it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.52it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.57it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:55,  2.30it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.48it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.89it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.82it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.90it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.56it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.60it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.32it/s]\u001b[A\n",
            " 28% 44/160 [00:17<00:53,  2.15it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.29it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.14it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.10it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.38it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:46,  2.41it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:44,  2.47it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:45,  2.39it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:49,  2.18it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:49,  2.15it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.35it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.50it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.84it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.82it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.81it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.51it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.35it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.45it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:42,  2.24it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:38,  2.44it/s]\u001b[A\n",
            " 41% 66/160 [00:26<00:35,  2.61it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.35it/s]\u001b[A\n",
            " 42% 68/160 [00:27<00:41,  2.24it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.56it/s]\u001b[A\n",
            " 44% 71/160 [00:28<00:32,  2.72it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.37it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.19it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.30it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.67it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.44it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.20it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:38,  2.12it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:34,  2.38it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:34,  2.32it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:34,  2.30it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:36,  2.15it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.29it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:33,  2.28it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.21it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.33it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.33it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.52it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.58it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.73it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.93it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.50it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.75it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.86it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:24,  2.66it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.46it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:23,  2.64it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.78it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.77it/s]\u001b[A\n",
            " 62% 100/160 [00:40<00:24,  2.50it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.44it/s]\u001b[A\n",
            " 64% 102/160 [00:41<00:27,  2.13it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.18it/s]\u001b[A\n",
            " 65% 104/160 [00:42<00:26,  2.08it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.06it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.26it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.35it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:24,  2.16it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.17it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:25,  1.99it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.94it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.85it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.04it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.98it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.23it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:48<00:18,  2.34it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.25it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.31it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:14,  2.42it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:13,  2.50it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.45it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.53it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.68it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.17it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.17it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.42it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.56it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.26it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.27it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.27it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.38it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.57it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.49it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.54it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.66it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.54it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.46it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.41it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.12it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.26it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.17it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.31it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.42it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.05it/s]\u001b[A\n",
            " 99% 159/160 [01:06<00:00,  2.12it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.03378294035792351, 'eval_runtime': 67.7296, 'eval_samples_per_second': 2.362, 'eval_steps_per_second': 2.362, 'eval_rewards/chosen': 1.44935142993927, 'eval_rewards/rejected': -3.721815586090088, 'eval_rewards/accuracies': 0.9937499761581421, 'eval_rewards/margins': 5.171166896820068, 'eval_logps/chosen': -198.11805725097656, 'eval_logps/rejected': -222.2740936279297, 'eval_logits/chosen': -0.9122945666313171, 'eval_logits/rejected': -0.8850879669189453, 'epoch': 2.89}\n",
            " 40% 40/100 [11:12<10:40, 10.67s/it]\n",
            "100% 160/160 [01:07<00:00,  2.14it/s]\u001b[A\n",
            "{'loss': 0.1357, 'grad_norm': 2.7540135383605957, 'learning_rate': 0.0002, 'rewards/chosen': 1.1514925956726074, 'rewards/rejected': -2.561835289001465, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 3.713327646255493, 'logps/chosen': -165.72344970703125, 'logps/rejected': -216.9426727294922, 'logits/chosen': -0.9247655272483826, 'logits/rejected': -0.8246652483940125, 'epoch': 2.96}\n",
            "{'loss': 0.0504, 'grad_norm': 2.6046581268310547, 'learning_rate': 0.000205, 'rewards/chosen': 0.7146018743515015, 'rewards/rejected': -4.413538932800293, 'rewards/accuracies': 1.0, 'rewards/margins': 5.128140926361084, 'logps/chosen': -314.73699951171875, 'logps/rejected': -237.90814208984375, 'logits/chosen': -0.880109429359436, 'logits/rejected': -0.7484962940216064, 'epoch': 3.0}\n",
            "{'loss': 0.0158, 'grad_norm': 0.4330824613571167, 'learning_rate': 0.00021, 'rewards/chosen': 2.353303909301758, 'rewards/rejected': -3.2243716716766357, 'rewards/accuracies': 1.0, 'rewards/margins': 5.577674865722656, 'logps/chosen': -214.84835815429688, 'logps/rejected': -154.8633270263672, 'logits/chosen': -0.9398428201675415, 'logits/rejected': -1.006413459777832, 'epoch': 3.07}\n",
            "{'loss': 0.0264, 'grad_norm': 1.0398145914077759, 'learning_rate': 0.000215, 'rewards/chosen': 0.3694530427455902, 'rewards/rejected': -5.341689109802246, 'rewards/accuracies': 1.0, 'rewards/margins': 5.711142539978027, 'logps/chosen': -190.12611389160156, 'logps/rejected': -279.6749267578125, 'logits/chosen': -0.9267380237579346, 'logits/rejected': -1.176127314567566, 'epoch': 3.15}\n",
            "{'loss': 0.0278, 'grad_norm': 1.828858733177185, 'learning_rate': 0.00022, 'rewards/chosen': 1.6520215272903442, 'rewards/rejected': -3.4638020992279053, 'rewards/accuracies': 1.0, 'rewards/margins': 5.115823745727539, 'logps/chosen': -265.08135986328125, 'logps/rejected': -153.07774353027344, 'logits/chosen': -0.7367776036262512, 'logits/rejected': -0.796343207359314, 'epoch': 3.22}\n",
            "{'loss': 0.0163, 'grad_norm': 0.6331427693367004, 'learning_rate': 0.00022500000000000002, 'rewards/chosen': 0.75385582447052, 'rewards/rejected': -4.885073184967041, 'rewards/accuracies': 1.0, 'rewards/margins': 5.63892936706543, 'logps/chosen': -163.63978576660156, 'logps/rejected': -212.61676025390625, 'logits/chosen': -1.1440374851226807, 'logits/rejected': -1.0795058012008667, 'epoch': 3.3}\n",
            "{'loss': 0.0112, 'grad_norm': 0.6986607313156128, 'learning_rate': 0.00023, 'rewards/chosen': 0.7620412111282349, 'rewards/rejected': -6.080981254577637, 'rewards/accuracies': 1.0, 'rewards/margins': 6.84302282333374, 'logps/chosen': -276.3355712890625, 'logps/rejected': -312.96990966796875, 'logits/chosen': -1.1192841529846191, 'logits/rejected': -1.1334997415542603, 'epoch': 3.37}\n",
            "{'loss': 0.0026, 'grad_norm': 0.18120411038398743, 'learning_rate': 0.000235, 'rewards/chosen': 1.6211838722229004, 'rewards/rejected': -5.531007766723633, 'rewards/accuracies': 1.0, 'rewards/margins': 7.152192115783691, 'logps/chosen': -200.028564453125, 'logps/rejected': -233.54733276367188, 'logits/chosen': -1.1614594459533691, 'logits/rejected': -1.016631841659546, 'epoch': 3.44}\n",
            "{'loss': 0.0035, 'grad_norm': 0.12083277106285095, 'learning_rate': 0.00024, 'rewards/chosen': 2.0820019245147705, 'rewards/rejected': -4.907647132873535, 'rewards/accuracies': 1.0, 'rewards/margins': 6.989648818969727, 'logps/chosen': -206.1224365234375, 'logps/rejected': -201.86053466796875, 'logits/chosen': -1.2268452644348145, 'logits/rejected': -1.236427903175354, 'epoch': 3.52}\n",
            "{'loss': 0.008, 'grad_norm': 0.23601526021957397, 'learning_rate': 0.000245, 'rewards/chosen': 0.6974585056304932, 'rewards/rejected': -7.332536697387695, 'rewards/accuracies': 1.0, 'rewards/margins': 8.02999496459961, 'logps/chosen': -146.88815307617188, 'logps/rejected': -315.8363952636719, 'logits/chosen': -0.9733555316925049, 'logits/rejected': -0.9620733857154846, 'epoch': 3.59}\n",
            " 50% 50/100 [12:45<08:58, 10.78s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:43,  3.60it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:59,  2.63it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:57,  2.74it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:07,  2.30it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:03,  2.43it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:06,  2.30it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.23it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.43it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.11it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.26it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.45it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.26it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.47it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.64it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:51,  2.68it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:55,  2.47it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.74it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:49,  2.74it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.75it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:58,  2.28it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.27it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.49it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:51,  2.53it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:57,  2.25it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:53,  2.41it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:50,  2.54it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:46,  2.68it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:46,  2.71it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.95it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.88it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.82it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.90it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:47,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.15it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.30it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.14it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.11it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:46,  2.39it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:44,  2.48it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:42,  2.56it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:43,  2.48it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:47,  2.26it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.20it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.41it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.38it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:40,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.84it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.76it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.71it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:40,  2.44it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:43,  2.27it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.39it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.19it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.40it/s]\u001b[A\n",
            " 41% 66/160 [00:26<00:36,  2.59it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.34it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.57it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.72it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.38it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.20it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:36,  2.33it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.70it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.46it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:36,  2.24it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.19it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.49it/s]\u001b[A\n",
            " 50% 80/160 [00:32<00:32,  2.45it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.42it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:34,  2.23it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.36it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.25it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.37it/s]\u001b[A\n",
            " 54% 87/160 [00:35<00:31,  2.31it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:29,  2.48it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:28,  2.52it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.64it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.80it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.43it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.69it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.82it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.44it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.62it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.77it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:40<00:24,  2.49it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.43it/s]\u001b[A\n",
            " 64% 102/160 [00:41<00:27,  2.13it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.19it/s]\u001b[A\n",
            " 65% 104/160 [00:42<00:27,  2.07it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.07it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.30it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.43it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.23it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.27it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.04it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.98it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.88it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.06it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.19it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:19,  2.31it/s]\u001b[A\n",
            " 73% 117/160 [00:48<00:19,  2.25it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.18it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:17,  2.39it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.20it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.22it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.43it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.28it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:15,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.48it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.34it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.52it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.66it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.21it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.50it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.67it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.35it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.42it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.61it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.52it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.56it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.35it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.56it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.59it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.46it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.33it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.43it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.39it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.16it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.31it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.43it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.06it/s]\u001b[A\n",
            " 99% 159/160 [01:06<00:00,  2.13it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.0037223093677312136, 'eval_runtime': 67.6136, 'eval_samples_per_second': 2.366, 'eval_steps_per_second': 2.366, 'eval_rewards/chosen': 1.4144307374954224, 'eval_rewards/rejected': -6.463156700134277, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 7.87758731842041, 'eval_logps/chosen': -198.46725463867188, 'eval_logps/rejected': -249.6875457763672, 'eval_logits/chosen': -1.154686689376831, 'eval_logits/rejected': -1.098694086074829, 'epoch': 3.59}\n",
            " 50% 50/100 [13:53<08:58, 10.78s/it]\n",
            "100% 160/160 [01:07<00:00,  2.19it/s]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 0.0033, 'grad_norm': 0.6649600267410278, 'learning_rate': 0.00025, 'rewards/chosen': 1.5335794687271118, 'rewards/rejected': -6.184887409210205, 'rewards/accuracies': 1.0, 'rewards/margins': 7.718466281890869, 'logps/chosen': -188.1094970703125, 'logps/rejected': -237.7847900390625, 'logits/chosen': -1.0812300443649292, 'logits/rejected': -0.9009645581245422, 'epoch': 3.67}\n",
            "{'loss': 0.0029, 'grad_norm': 0.17978432774543762, 'learning_rate': 0.000255, 'rewards/chosen': 0.6195282936096191, 'rewards/rejected': -7.894464492797852, 'rewards/accuracies': 1.0, 'rewards/margins': 8.513993263244629, 'logps/chosen': -168.1539764404297, 'logps/rejected': -323.22869873046875, 'logits/chosen': -1.131040334701538, 'logits/rejected': -1.1394484043121338, 'epoch': 3.74}\n",
            "{'loss': 0.0064, 'grad_norm': 0.259136438369751, 'learning_rate': 0.00026000000000000003, 'rewards/chosen': 0.19270405173301697, 'rewards/rejected': -7.795896530151367, 'rewards/accuracies': 1.0, 'rewards/margins': 7.98859977722168, 'logps/chosen': -238.01034545898438, 'logps/rejected': -324.7440185546875, 'logits/chosen': -1.201636791229248, 'logits/rejected': -0.9111640453338623, 'epoch': 3.81}\n",
            "{'loss': 0.0127, 'grad_norm': 0.32084816694259644, 'learning_rate': 0.00026500000000000004, 'rewards/chosen': 1.6130621433258057, 'rewards/rejected': -5.840975284576416, 'rewards/accuracies': 1.0, 'rewards/margins': 7.454036712646484, 'logps/chosen': -195.8231658935547, 'logps/rejected': -208.25534057617188, 'logits/chosen': -1.167710781097412, 'logits/rejected': -1.1877844333648682, 'epoch': 3.89}\n",
            "{'loss': 0.0041, 'grad_norm': 0.22193589806556702, 'learning_rate': 0.00027, 'rewards/chosen': 0.9431224465370178, 'rewards/rejected': -7.027093410491943, 'rewards/accuracies': 1.0, 'rewards/margins': 7.970215797424316, 'logps/chosen': -170.81048583984375, 'logps/rejected': -197.21405029296875, 'logits/chosen': -1.1650159358978271, 'logits/rejected': -1.2898290157318115, 'epoch': 3.96}\n",
            "{'loss': 0.0001, 'grad_norm': 0.013202623464167118, 'learning_rate': 0.000275, 'rewards/chosen': -0.1503440886735916, 'rewards/rejected': -11.69331169128418, 'rewards/accuracies': 1.0, 'rewards/margins': 11.54296875, 'logps/chosen': -254.66067504882812, 'logps/rejected': -451.62640380859375, 'logits/chosen': -1.7795112133026123, 'logits/rejected': -1.7332942485809326, 'epoch': 4.0}\n",
            "{'loss': 0.0016, 'grad_norm': 0.08065909147262573, 'learning_rate': 0.00028000000000000003, 'rewards/chosen': 0.7970767021179199, 'rewards/rejected': -7.952095985412598, 'rewards/accuracies': 1.0, 'rewards/margins': 8.749173164367676, 'logps/chosen': -210.8603973388672, 'logps/rejected': -276.297119140625, 'logits/chosen': -1.4927257299423218, 'logits/rejected': -1.3283195495605469, 'epoch': 4.07}\n",
            "{'loss': 0.0024, 'grad_norm': 0.1036483645439148, 'learning_rate': 0.000285, 'rewards/chosen': 1.2142335176467896, 'rewards/rejected': -7.80238151550293, 'rewards/accuracies': 1.0, 'rewards/margins': 9.01661491394043, 'logps/chosen': -197.62721252441406, 'logps/rejected': -267.7578125, 'logits/chosen': -1.3989018201828003, 'logits/rejected': -1.3003097772598267, 'epoch': 4.15}\n",
            "{'loss': 0.0017, 'grad_norm': 0.09124814718961716, 'learning_rate': 0.00029, 'rewards/chosen': 1.1163966655731201, 'rewards/rejected': -7.738553047180176, 'rewards/accuracies': 1.0, 'rewards/margins': 8.854949951171875, 'logps/chosen': -217.74169921875, 'logps/rejected': -250.56597900390625, 'logits/chosen': -1.4494478702545166, 'logits/rejected': -1.5571670532226562, 'epoch': 4.22}\n",
            "{'loss': 0.0002, 'grad_norm': 0.0115854786708951, 'learning_rate': 0.000295, 'rewards/chosen': 0.42328304052352905, 'rewards/rejected': -10.146357536315918, 'rewards/accuracies': 1.0, 'rewards/margins': 10.569640159606934, 'logps/chosen': -185.39683532714844, 'logps/rejected': -285.26043701171875, 'logits/chosen': -1.2379528284072876, 'logits/rejected': -1.3338249921798706, 'epoch': 4.3}\n",
            " 60% 60/100 [15:30<06:59, 10.49s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.82it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.76it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:55,  2.82it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:06,  2.34it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:03,  2.42it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:08,  2.24it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:09,  2.19it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:07,  2.23it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:12,  2.08it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:04,  2.31it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.43it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:04,  2.29it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.12it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.26it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.47it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.27it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.47it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.64it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.67it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:55,  2.47it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:49,  2.72it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.72it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.26it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.03it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.26it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.53it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.60it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:55,  2.32it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:52,  2.46it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:49,  2.58it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:47,  2.63it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:45,  2.74it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:45,  2.73it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:44,  2.71it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:42,  2.81it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.62it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:47,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.14it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.29it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:53,  2.13it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.37it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.52it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.44it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.23it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:49,  2.18it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.39it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:40,  2.52it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.86it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.84it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.82it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.48it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:42,  2.31it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.38it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:44,  2.16it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.38it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.57it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.56it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.35it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.18it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.30it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.67it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.46it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.24it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.19it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.49it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.44it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.42it/s]\u001b[A\n",
            " 51% 82/160 [00:33<00:35,  2.23it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.36it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.35it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.25it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.37it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:30,  2.36it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.56it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.56it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.67it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.84it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.44it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:25,  2.63it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:24,  2.75it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:25,  2.58it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.40it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.58it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.73it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.70it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.46it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.12it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.18it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.07it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.07it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.30it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.43it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.22it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.26it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.02it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.96it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.87it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.05it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.22it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.34it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.28it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.19it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:17,  2.36it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.16it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.19it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.41it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.29it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.50it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.45it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.53it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.70it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.38it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.22it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.50it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.68it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.36it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.34it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.34it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.43it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.61it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.52it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.56it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.39it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.63it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.61it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.48it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.31it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:05,  2.39it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.31it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.07it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.21it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.08it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.29it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.40it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.05it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.13it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.0007009954424574971, 'eval_runtime': 67.7849, 'eval_samples_per_second': 2.36, 'eval_steps_per_second': 2.36, 'eval_rewards/chosen': 0.5990064144134521, 'eval_rewards/rejected': -9.313703536987305, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 9.912711143493652, 'eval_logps/chosen': -206.6215362548828, 'eval_logps/rejected': -278.1929931640625, 'eval_logits/chosen': -1.5036365985870361, 'eval_logits/rejected': -1.4425523281097412, 'epoch': 4.3}\n",
            " 60% 60/100 [16:38<06:59, 10.49s/it]\n",
            "100% 160/160 [01:07<00:00,  2.20it/s]\u001b[A\n",
            "{'loss': 0.0003, 'grad_norm': 0.0178285613656044, 'learning_rate': 0.0003, 'rewards/chosen': 1.1132090091705322, 'rewards/rejected': -7.936180114746094, 'rewards/accuracies': 1.0, 'rewards/margins': 9.049388885498047, 'logps/chosen': -189.77728271484375, 'logps/rejected': -206.59307861328125, 'logits/chosen': -1.68360435962677, 'logits/rejected': -1.723506212234497, 'epoch': 4.37}\n",
            "{'loss': 0.0002, 'grad_norm': 0.010418657213449478, 'learning_rate': 0.000305, 'rewards/chosen': 1.6600487232208252, 'rewards/rejected': -8.199295043945312, 'rewards/accuracies': 1.0, 'rewards/margins': 9.859342575073242, 'logps/chosen': -190.26272583007812, 'logps/rejected': -260.24310302734375, 'logits/chosen': -1.3131120204925537, 'logits/rejected': -1.4500433206558228, 'epoch': 4.44}\n",
            "{'loss': 0.0001, 'grad_norm': 0.008662802167236805, 'learning_rate': 0.00031, 'rewards/chosen': -1.1442416906356812, 'rewards/rejected': -12.82375717163086, 'rewards/accuracies': 1.0, 'rewards/margins': 11.679515838623047, 'logps/chosen': -188.37603759765625, 'logps/rejected': -384.220458984375, 'logits/chosen': -1.7190930843353271, 'logits/rejected': -1.350312352180481, 'epoch': 4.52}\n",
            "{'loss': 0.0006, 'grad_norm': 0.06184183061122894, 'learning_rate': 0.000315, 'rewards/chosen': -0.043107688426971436, 'rewards/rejected': -9.704010009765625, 'rewards/accuracies': 1.0, 'rewards/margins': 9.66090202331543, 'logps/chosen': -301.73870849609375, 'logps/rejected': -268.00433349609375, 'logits/chosen': -1.7276580333709717, 'logits/rejected': -1.730417013168335, 'epoch': 4.59}\n",
            "{'loss': 0.0002, 'grad_norm': 0.015530990436673164, 'learning_rate': 0.00032, 'rewards/chosen': 0.8674333095550537, 'rewards/rejected': -10.816995620727539, 'rewards/accuracies': 1.0, 'rewards/margins': 11.684428215026855, 'logps/chosen': -207.59043884277344, 'logps/rejected': -325.4638671875, 'logits/chosen': -1.6748735904693604, 'logits/rejected': -1.4124407768249512, 'epoch': 4.67}\n",
            "{'loss': 0.002, 'grad_norm': 0.13642659783363342, 'learning_rate': 0.00032500000000000004, 'rewards/chosen': 2.4750452041625977, 'rewards/rejected': -6.512556552886963, 'rewards/accuracies': 1.0, 'rewards/margins': 8.987601280212402, 'logps/chosen': -176.35475158691406, 'logps/rejected': -165.11550903320312, 'logits/chosen': -1.6565958261489868, 'logits/rejected': -1.381290316581726, 'epoch': 4.74}\n",
            "{'loss': 0.0001, 'grad_norm': 0.007178422063589096, 'learning_rate': 0.00033, 'rewards/chosen': -1.586328148841858, 'rewards/rejected': -14.042205810546875, 'rewards/accuracies': 1.0, 'rewards/margins': 12.455877304077148, 'logps/chosen': -236.728515625, 'logps/rejected': -408.80108642578125, 'logits/chosen': -1.4989510774612427, 'logits/rejected': -1.5772340297698975, 'epoch': 4.81}\n",
            "{'loss': 0.0002, 'grad_norm': 0.014228942804038525, 'learning_rate': 0.000335, 'rewards/chosen': -0.2139132022857666, 'rewards/rejected': -11.67084789276123, 'rewards/accuracies': 1.0, 'rewards/margins': 11.456933975219727, 'logps/chosen': -178.69100952148438, 'logps/rejected': -288.1889343261719, 'logits/chosen': -1.8483805656433105, 'logits/rejected': -1.676049828529358, 'epoch': 4.89}\n",
            "{'loss': 0.0006, 'grad_norm': 0.07336296141147614, 'learning_rate': 0.00034, 'rewards/chosen': -0.06068915128707886, 'rewards/rejected': -9.90105152130127, 'rewards/accuracies': 1.0, 'rewards/margins': 9.840362548828125, 'logps/chosen': -149.27645874023438, 'logps/rejected': -250.3993682861328, 'logits/chosen': -1.7224185466766357, 'logits/rejected': -1.7093087434768677, 'epoch': 4.96}\n",
            "{'loss': 0.0139, 'grad_norm': 3.5430703163146973, 'learning_rate': 0.000345, 'rewards/chosen': -2.4785022735595703, 'rewards/rejected': -13.852887153625488, 'rewards/accuracies': 1.0, 'rewards/margins': 11.374384880065918, 'logps/chosen': -228.6499481201172, 'logps/rejected': -436.65850830078125, 'logits/chosen': -1.4845969676971436, 'logits/rejected': -1.577852487564087, 'epoch': 5.0}\n",
            " 70% 70/100 [18:16<04:40,  9.34s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.74it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:58,  2.67it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:56,  2.74it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:07,  2.31it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:04,  2.39it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:07,  2.27it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.21it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:07,  2.25it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.45it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.10it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.25it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.46it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.26it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.46it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.63it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.64it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.43it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.70it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.69it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.71it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.26it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.25it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.53it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:51,  2.55it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:57,  2.25it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:52,  2.42it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:49,  2.56it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:47,  2.64it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:46,  2.67it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.91it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:43,  2.85it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.80it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.89it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.14it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.28it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:53,  2.12it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:53,  2.10it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.38it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.44it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.46it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.24it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.19it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.40it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:40,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.86it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.80it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.72it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:40,  2.43it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:42,  2.28it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:41,  2.36it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:44,  2.17it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.38it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.57it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.31it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.17it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.71it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.37it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.19it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.31it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.68it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.46it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.24it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.20it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.49it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.45it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.40it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.22it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.33it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.25it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.36it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.32it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.49it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:28,  2.51it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.64it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.80it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:28,  2.41it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:25,  2.67it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.79it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.42it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.60it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.75it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.75it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.43it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.13it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.18it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.07it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.06it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.29it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.41it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.23it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.27it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.05it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.99it/s]\u001b[A\n",
            " 70% 112/160 [00:46<00:25,  1.88it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.06it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.99it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.20it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.32it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.28it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.18it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:17,  2.40it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.21it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.24it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:15,  2.32it/s]\u001b[A\n",
            " 78% 124/160 [00:51<00:14,  2.42it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:13,  2.51it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.37it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.46it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.54it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.70it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.39it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.38it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.22it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.50it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.66it/s]\u001b[A\n",
            " 85% 136/160 [00:56<00:10,  2.35it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.42it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.61it/s]\u001b[A\n",
            " 88% 141/160 [00:58<00:07,  2.52it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.57it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.38it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.58it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.60it/s]\u001b[A\n",
            " 91% 146/160 [01:00<00:05,  2.47it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.28it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.40it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.37it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.24it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.09it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.29it/s]\u001b[A\n",
            " 97% 155/160 [01:04<00:02,  2.39it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.05it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.12it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.00018145234207622707, 'eval_runtime': 67.7314, 'eval_samples_per_second': 2.362, 'eval_steps_per_second': 2.362, 'eval_rewards/chosen': 0.4006687104701996, 'eval_rewards/rejected': -10.669858932495117, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 11.070528030395508, 'eval_logps/chosen': -208.60488891601562, 'eval_logps/rejected': -291.7545471191406, 'eval_logits/chosen': -1.677943229675293, 'eval_logits/rejected': -1.6246029138565063, 'epoch': 5.0}\n",
            " 70% 70/100 [19:24<04:40,  9.34s/it]\n",
            "100% 160/160 [01:07<00:00,  2.19it/s]\u001b[A\n",
            "{'loss': 0.0001, 'grad_norm': 0.0047614178620278835, 'learning_rate': 0.00035, 'rewards/chosen': 2.0375800132751465, 'rewards/rejected': -9.547292709350586, 'rewards/accuracies': 1.0, 'rewards/margins': 11.584872245788574, 'logps/chosen': -160.66143798828125, 'logps/rejected': -251.09420776367188, 'logits/chosen': -1.6490845680236816, 'logits/rejected': -1.7058227062225342, 'epoch': 5.07}\n",
            "{'loss': 0.0001, 'grad_norm': 0.003635255852714181, 'learning_rate': 0.000355, 'rewards/chosen': 0.18032163381576538, 'rewards/rejected': -13.865942001342773, 'rewards/accuracies': 1.0, 'rewards/margins': 14.0462646484375, 'logps/chosen': -313.7747802734375, 'logps/rejected': -431.271484375, 'logits/chosen': -1.538459062576294, 'logits/rejected': -1.5810092687606812, 'epoch': 5.15}\n",
            "{'loss': 0.0001, 'grad_norm': 0.005686794873327017, 'learning_rate': 0.00035999999999999997, 'rewards/chosen': 0.9796962141990662, 'rewards/rejected': -10.476582527160645, 'rewards/accuracies': 1.0, 'rewards/margins': 11.456278800964355, 'logps/chosen': -190.38035583496094, 'logps/rejected': -274.47161865234375, 'logits/chosen': -1.6712359189987183, 'logits/rejected': -1.4574322700500488, 'epoch': 5.22}\n",
            "{'loss': 0.0002, 'grad_norm': 0.012322739697992802, 'learning_rate': 0.000365, 'rewards/chosen': 2.032280921936035, 'rewards/rejected': -8.917230606079102, 'rewards/accuracies': 1.0, 'rewards/margins': 10.949512481689453, 'logps/chosen': -223.73060607910156, 'logps/rejected': -293.3909912109375, 'logits/chosen': -1.6409634351730347, 'logits/rejected': -1.4457019567489624, 'epoch': 5.3}\n",
            "{'loss': 0.0001, 'grad_norm': 0.006878133863210678, 'learning_rate': 0.00037, 'rewards/chosen': 2.1884751319885254, 'rewards/rejected': -7.76554012298584, 'rewards/accuracies': 1.0, 'rewards/margins': 9.954014778137207, 'logps/chosen': -157.67990112304688, 'logps/rejected': -209.48878479003906, 'logits/chosen': -1.792372226715088, 'logits/rejected': -1.7092881202697754, 'epoch': 5.37}\n",
            "{'loss': 0.0004, 'grad_norm': 0.025643596425652504, 'learning_rate': 0.000375, 'rewards/chosen': 0.7707303762435913, 'rewards/rejected': -10.803028106689453, 'rewards/accuracies': 1.0, 'rewards/margins': 11.57375717163086, 'logps/chosen': -246.51925659179688, 'logps/rejected': -318.9184875488281, 'logits/chosen': -1.3447248935699463, 'logits/rejected': -1.3398785591125488, 'epoch': 5.44}\n",
            "{'loss': 0.0003, 'grad_norm': 0.019759122282266617, 'learning_rate': 0.00038, 'rewards/chosen': 0.5258139371871948, 'rewards/rejected': -8.900773048400879, 'rewards/accuracies': 1.0, 'rewards/margins': 9.426587104797363, 'logps/chosen': -187.17718505859375, 'logps/rejected': -243.77606201171875, 'logits/chosen': -1.7031729221343994, 'logits/rejected': -1.710444450378418, 'epoch': 5.52}\n",
            "{'loss': 0.0002, 'grad_norm': 0.009069357067346573, 'learning_rate': 0.00038500000000000003, 'rewards/chosen': 0.7668923735618591, 'rewards/rejected': -8.499639511108398, 'rewards/accuracies': 1.0, 'rewards/margins': 9.266531944274902, 'logps/chosen': -228.16317749023438, 'logps/rejected': -268.3163146972656, 'logits/chosen': -1.582854986190796, 'logits/rejected': -1.6245437860488892, 'epoch': 5.59}\n",
            "{'loss': 0.0004, 'grad_norm': 0.019039344042539597, 'learning_rate': 0.00039000000000000005, 'rewards/chosen': 0.8357537984848022, 'rewards/rejected': -8.982575416564941, 'rewards/accuracies': 1.0, 'rewards/margins': 9.818328857421875, 'logps/chosen': -254.4918212890625, 'logps/rejected': -306.6686096191406, 'logits/chosen': -1.659993290901184, 'logits/rejected': -1.6295599937438965, 'epoch': 5.67}\n",
            "{'loss': 0.0007, 'grad_norm': 0.03336164727807045, 'learning_rate': 0.000395, 'rewards/chosen': -0.4248476028442383, 'rewards/rejected': -10.074069023132324, 'rewards/accuracies': 1.0, 'rewards/margins': 9.649221420288086, 'logps/chosen': -165.27154541015625, 'logps/rejected': -264.883544921875, 'logits/chosen': -1.6940408945083618, 'logits/rejected': -1.678821086883545, 'epoch': 5.74}\n",
            " 80% 80/100 [21:10<03:42, 11.13s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.81it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.77it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.88it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.39it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.32it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:04,  2.31it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:02,  2.37it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:05,  2.23it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:10,  2.07it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:10,  2.06it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:05,  2.21it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:59,  2.42it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.24it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.44it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.61it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.65it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.46it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.72it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:49,  2.73it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.74it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:58,  2.27it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.27it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.54it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.58it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:55,  2.31it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.49it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.78it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.89it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.82it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.91it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:47,  2.50it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:47,  2.50it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:52,  2.24it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:56,  2.06it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:51,  2.22it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:54,  2.09it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.06it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.35it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.44it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.45it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.25it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:48,  2.20it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.40it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.36it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.50it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.82it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.81it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.79it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.50it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.34it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.44it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.23it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.43it/s]\u001b[A\n",
            " 41% 66/160 [00:26<00:36,  2.61it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.34it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.23it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:42,  2.15it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:36,  2.49it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:34,  2.61it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:38,  2.28it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:41,  2.10it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:38,  2.25it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:32,  2.61it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.41it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.21it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.43it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.40it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.20it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.32it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.23it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.36it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.35it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.54it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.61it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.75it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.95it/s]\u001b[A\n",
            " 57% 92/160 [00:37<00:27,  2.51it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.75it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.87it/s]\u001b[A\n",
            " 59% 95/160 [00:38<00:24,  2.64it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.44it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.62it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.73it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:23,  2.65it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:25,  2.38it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:25,  2.29it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:28,  2.02it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:27,  2.09it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.01it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:27,  1.99it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:24,  2.23it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.37it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.20it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.24it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.03it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.97it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:25,  1.87it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.05it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.98it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.24it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.37it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.35it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.25it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.25it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.31it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.45it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.27it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:14,  2.34it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:13,  2.41it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:12,  2.58it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.32it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.30it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.14it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.47it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.64it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.32it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.40it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.59it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.47it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.53it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.37it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.63it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.68it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.57it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.48it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.43it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.13it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.27it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.14it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.25it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.31it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:02,  1.98it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.00it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.00it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.09it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.000248302414547652, 'eval_runtime': 67.9434, 'eval_samples_per_second': 2.355, 'eval_steps_per_second': 2.355, 'eval_rewards/chosen': 1.231394648551941, 'eval_rewards/rejected': -9.1537446975708, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 10.385138511657715, 'eval_logps/chosen': -200.297607421875, 'eval_logps/rejected': -276.5934143066406, 'eval_logits/chosen': -1.6101577281951904, 'eval_logits/rejected': -1.5643101930618286, 'epoch': 5.74}\n",
            " 80% 80/100 [22:18<03:42, 11.13s/it]\n",
            "100% 160/160 [01:07<00:00,  2.16it/s]\u001b[A\n",
            "{'loss': 0.0002, 'grad_norm': 0.009920509532094002, 'learning_rate': 0.0004, 'rewards/chosen': 1.1013481616973877, 'rewards/rejected': -9.567852973937988, 'rewards/accuracies': 1.0, 'rewards/margins': 10.669200897216797, 'logps/chosen': -169.96743774414062, 'logps/rejected': -308.3231506347656, 'logits/chosen': -1.829941749572754, 'logits/rejected': -1.4346373081207275, 'epoch': 5.81}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00985016580671072, 'learning_rate': 0.00040500000000000003, 'rewards/chosen': 0.5823160409927368, 'rewards/rejected': -9.488302230834961, 'rewards/accuracies': 1.0, 'rewards/margins': 10.070618629455566, 'logps/chosen': -185.97225952148438, 'logps/rejected': -282.52239990234375, 'logits/chosen': -1.7722073793411255, 'logits/rejected': -1.6924704313278198, 'epoch': 5.89}\n",
            "{'loss': 0.0004, 'grad_norm': 0.01445756759494543, 'learning_rate': 0.00041, 'rewards/chosen': 1.0257844924926758, 'rewards/rejected': -7.967942237854004, 'rewards/accuracies': 1.0, 'rewards/margins': 8.99372673034668, 'logps/chosen': -143.53189086914062, 'logps/rejected': -219.2347412109375, 'logits/chosen': -1.4981993436813354, 'logits/rejected': -1.389601707458496, 'epoch': 5.96}\n",
            "{'loss': 0.0003, 'grad_norm': 0.03710709512233734, 'learning_rate': 0.000415, 'rewards/chosen': 2.993591785430908, 'rewards/rejected': -6.598224639892578, 'rewards/accuracies': 1.0, 'rewards/margins': 9.591815948486328, 'logps/chosen': -227.135009765625, 'logps/rejected': -165.57662963867188, 'logits/chosen': -1.6096575260162354, 'logits/rejected': -1.2914462089538574, 'epoch': 6.0}\n",
            "{'loss': 0.0002, 'grad_norm': 0.010109728202223778, 'learning_rate': 0.00042, 'rewards/chosen': 1.2218787670135498, 'rewards/rejected': -8.199701309204102, 'rewards/accuracies': 1.0, 'rewards/margins': 9.42158031463623, 'logps/chosen': -264.3200378417969, 'logps/rejected': -253.39852905273438, 'logits/chosen': -1.166213035583496, 'logits/rejected': -1.5852882862091064, 'epoch': 6.07}\n",
            "{'loss': 0.0001, 'grad_norm': 0.007682550232857466, 'learning_rate': 0.000425, 'rewards/chosen': 1.1793216466903687, 'rewards/rejected': -10.128019332885742, 'rewards/accuracies': 1.0, 'rewards/margins': 11.307341575622559, 'logps/chosen': -303.83172607421875, 'logps/rejected': -384.7288818359375, 'logits/chosen': -1.3897286653518677, 'logits/rejected': -1.4263083934783936, 'epoch': 6.15}\n",
            "{'loss': 0.0001, 'grad_norm': 0.008294880390167236, 'learning_rate': 0.00043, 'rewards/chosen': 1.1241048574447632, 'rewards/rejected': -9.611177444458008, 'rewards/accuracies': 1.0, 'rewards/margins': 10.735282897949219, 'logps/chosen': -211.24514770507812, 'logps/rejected': -347.6710510253906, 'logits/chosen': -1.7276474237442017, 'logits/rejected': -1.6052923202514648, 'epoch': 6.22}\n",
            "{'loss': 0.0002, 'grad_norm': 0.008579503744840622, 'learning_rate': 0.000435, 'rewards/chosen': 2.0019209384918213, 'rewards/rejected': -8.106563568115234, 'rewards/accuracies': 1.0, 'rewards/margins': 10.108484268188477, 'logps/chosen': -153.4726104736328, 'logps/rejected': -226.46556091308594, 'logits/chosen': -1.8714330196380615, 'logits/rejected': -1.7578184604644775, 'epoch': 6.3}\n",
            "{'loss': 0.0004, 'grad_norm': 0.01465094555169344, 'learning_rate': 0.00044, 'rewards/chosen': 1.7938659191131592, 'rewards/rejected': -8.406814575195312, 'rewards/accuracies': 1.0, 'rewards/margins': 10.200681686401367, 'logps/chosen': -190.93780517578125, 'logps/rejected': -229.1738739013672, 'logits/chosen': -1.650373935699463, 'logits/rejected': -1.6368999481201172, 'epoch': 6.37}\n",
            "{'loss': 0.0003, 'grad_norm': 0.013166707009077072, 'learning_rate': 0.00044500000000000003, 'rewards/chosen': 1.1184039115905762, 'rewards/rejected': -8.33173942565918, 'rewards/accuracies': 1.0, 'rewards/margins': 9.450141906738281, 'logps/chosen': -205.16326904296875, 'logps/rejected': -232.84036254882812, 'logits/chosen': -1.5924859046936035, 'logits/rejected': -1.4996590614318848, 'epoch': 6.44}\n",
            " 90% 90/100 [23:54<01:51, 11.17s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.74it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.74it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.86it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.40it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.34it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.25it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.14it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.36it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.31it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:09,  2.10it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:11,  2.04it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:06,  2.16it/s]\u001b[A\n",
            " 11% 17/160 [00:07<01:01,  2.33it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:05,  2.17it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:59,  2.39it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:54,  2.57it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:53,  2.61it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.42it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.67it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.71it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.26it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.04it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.26it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:51,  2.54it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.59it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:55,  2.31it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.50it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.65it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.89it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.56it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.59it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.32it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:55,  2.09it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:51,  2.22it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:55,  2.05it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:55,  2.04it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:48,  2.33it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.43it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.52it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:45,  2.42it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.22it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:49,  2.17it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.38it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.34it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.43it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.49it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.83it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.81it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.78it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.52it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.36it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.45it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:42,  2.23it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.44it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.61it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.34it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.24it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.56it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.72it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.33it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.15it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:38,  2.25it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:33,  2.56it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:36,  2.32it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:38,  2.17it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:38,  2.13it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:33,  2.43it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.39it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.39it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.21it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.33it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.24it/s]\u001b[A\n",
            " 54% 86/160 [00:36<00:39,  1.88it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:36,  1.98it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:32,  2.23it/s]\u001b[A\n",
            " 56% 89/160 [00:37<00:30,  2.36it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:27,  2.56it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:24,  2.81it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.43it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:25,  2.67it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.80it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.42it/s]\u001b[A\n",
            " 61% 97/160 [00:40<00:24,  2.61it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.47it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:25,  2.36it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:28,  2.06it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:27,  2.09it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.02it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:27,  2.03it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:23,  2.26it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.39it/s]\u001b[A\n",
            " 68% 108/160 [00:45<00:23,  2.21it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.26it/s]\u001b[A\n",
            " 69% 110/160 [00:46<00:24,  2.04it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:24,  1.97it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:25,  1.87it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:22,  2.05it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.99it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.24it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.25it/s]\u001b[A\n",
            " 74% 119/160 [00:50<00:16,  2.46it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.22it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.23it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.44it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.30it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.49it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.29it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:14,  2.35it/s]\u001b[A\n",
            " 80% 128/160 [00:54<00:13,  2.42it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:12,  2.55it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:13,  2.28it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.30it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.13it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.17it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.46it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.64it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.33it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.31it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.41it/s]\u001b[A\n",
            " 88% 140/160 [00:59<00:07,  2.59it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.51it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.56it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.67it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.55it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.45it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.26it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.18it/s]\u001b[A\n",
            " 96% 154/160 [01:05<00:02,  2.27it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.34it/s]\u001b[A\n",
            " 98% 156/160 [01:06<00:02,  2.00it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  1.96it/s]\u001b[A\n",
            " 99% 158/160 [01:07<00:01,  1.98it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.07it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.00018210953567177057, 'eval_runtime': 68.31, 'eval_samples_per_second': 2.342, 'eval_steps_per_second': 2.342, 'eval_rewards/chosen': 1.4108314514160156, 'eval_rewards/rejected': -9.106260299682617, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 10.517090797424316, 'eval_logps/chosen': -198.50326538085938, 'eval_logps/rejected': -276.1185607910156, 'eval_logits/chosen': -1.6663494110107422, 'eval_logits/rejected': -1.6275966167449951, 'epoch': 6.44}\n",
            " 90% 90/100 [25:03<01:51, 11.17s/it]\n",
            "100% 160/160 [01:08<00:00,  2.14it/s]\u001b[A\n",
            "{'loss': 0.0001, 'grad_norm': 0.004645145032554865, 'learning_rate': 0.00045000000000000004, 'rewards/chosen': 1.5197257995605469, 'rewards/rejected': -8.427045822143555, 'rewards/accuracies': 1.0, 'rewards/margins': 9.946771621704102, 'logps/chosen': -117.29981994628906, 'logps/rejected': -242.06961059570312, 'logits/chosen': -1.8213915824890137, 'logits/rejected': -1.8221756219863892, 'epoch': 6.52}\n",
            "{'loss': 0.0002, 'grad_norm': 0.010945308953523636, 'learning_rate': 0.000455, 'rewards/chosen': 0.5224724411964417, 'rewards/rejected': -9.64897632598877, 'rewards/accuracies': 1.0, 'rewards/margins': 10.17144775390625, 'logps/chosen': -229.61196899414062, 'logps/rejected': -344.3804931640625, 'logits/chosen': -1.9551366567611694, 'logits/rejected': -1.9791069030761719, 'epoch': 6.59}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00851000752300024, 'learning_rate': 0.00046, 'rewards/chosen': 1.167831301689148, 'rewards/rejected': -9.758475303649902, 'rewards/accuracies': 1.0, 'rewards/margins': 10.926305770874023, 'logps/chosen': -221.05084228515625, 'logps/rejected': -259.1885986328125, 'logits/chosen': -1.676059603691101, 'logits/rejected': -1.7492425441741943, 'epoch': 6.67}\n",
            "{'loss': 0.0, 'grad_norm': 0.00393592519685626, 'learning_rate': 0.000465, 'rewards/chosen': 3.255333662033081, 'rewards/rejected': -12.197717666625977, 'rewards/accuracies': 1.0, 'rewards/margins': 15.453051567077637, 'logps/chosen': -181.8994903564453, 'logps/rejected': -328.5892333984375, 'logits/chosen': -1.5719753503799438, 'logits/rejected': -1.3517844676971436, 'epoch': 6.74}\n",
            "{'loss': 0.0001, 'grad_norm': 0.008249257691204548, 'learning_rate': 0.00047, 'rewards/chosen': 1.3805586099624634, 'rewards/rejected': -9.315678596496582, 'rewards/accuracies': 1.0, 'rewards/margins': 10.696237564086914, 'logps/chosen': -158.44825744628906, 'logps/rejected': -239.83636474609375, 'logits/chosen': -2.0216116905212402, 'logits/rejected': -1.680154800415039, 'epoch': 6.81}\n",
            "{'loss': 0.0005, 'grad_norm': 0.016770929098129272, 'learning_rate': 0.000475, 'rewards/chosen': 2.485590696334839, 'rewards/rejected': -5.7855401039123535, 'rewards/accuracies': 1.0, 'rewards/margins': 8.271130561828613, 'logps/chosen': -141.60824584960938, 'logps/rejected': -153.02537536621094, 'logits/chosen': -1.5358479022979736, 'logits/rejected': -1.5597708225250244, 'epoch': 6.89}\n",
            "{'loss': 0.0001, 'grad_norm': 0.004959969315677881, 'learning_rate': 0.00048, 'rewards/chosen': 0.18467964231967926, 'rewards/rejected': -10.842757225036621, 'rewards/accuracies': 1.0, 'rewards/margins': 11.027436256408691, 'logps/chosen': -197.0322723388672, 'logps/rejected': -328.14080810546875, 'logits/chosen': -1.6656056642532349, 'logits/rejected': -1.5076713562011719, 'epoch': 6.96}\n",
            "{'loss': 0.0002, 'grad_norm': 0.02395014651119709, 'learning_rate': 0.00048499999999999997, 'rewards/chosen': -1.406229019165039, 'rewards/rejected': -11.832267761230469, 'rewards/accuracies': 1.0, 'rewards/margins': 10.42603874206543, 'logps/chosen': -191.1319122314453, 'logps/rejected': -368.56829833984375, 'logits/chosen': -1.914269208908081, 'logits/rejected': -1.3248815536499023, 'epoch': 7.0}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0036169751547276974, 'learning_rate': 0.00049, 'rewards/chosen': 1.1612472534179688, 'rewards/rejected': -9.60667610168457, 'rewards/accuracies': 1.0, 'rewards/margins': 10.767923355102539, 'logps/chosen': -202.68405151367188, 'logps/rejected': -259.42095947265625, 'logits/chosen': -1.868029236793518, 'logits/rejected': -1.6573575735092163, 'epoch': 7.07}\n",
            "{'loss': 0.0002, 'grad_norm': 0.006754860281944275, 'learning_rate': 0.000495, 'rewards/chosen': 1.6179203987121582, 'rewards/rejected': -8.543087005615234, 'rewards/accuracies': 1.0, 'rewards/margins': 10.161006927490234, 'logps/chosen': -198.79217529296875, 'logps/rejected': -267.3345642089844, 'logits/chosen': -1.8645216226577759, 'logits/rejected': -1.8516249656677246, 'epoch': 7.15}\n",
            "100% 100/100 [26:36<00:00, 10.06s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.81it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.79it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:53,  2.90it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.41it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.51it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:04,  2.36it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.26it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.35it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.12it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.26it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:57,  2.47it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:02,  2.26it/s]\u001b[A\n",
            " 12% 19/160 [00:07<00:57,  2.46it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.63it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.66it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.44it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.67it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:51,  2.63it/s]\u001b[A\n",
            " 16% 26/160 [00:10<01:01,  2.20it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:07,  1.96it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:59,  2.20it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.49it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.56it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:56,  2.30it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.48it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.65it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:46,  2.70it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.92it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:43,  2.86it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:44,  2.76it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:42,  2.86it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.64it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:47,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:51,  2.28it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.13it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.27it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.13it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.51it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.44it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.22it/s]\u001b[A\n",
            " 33% 53/160 [00:21<00:50,  2.12it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:45,  2.31it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:46,  2.27it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:44,  2.34it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.78it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.79it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.76it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.51it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.36it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.46it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.34it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.18it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:33,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.36it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.17it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.30it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.67it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.44it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.24it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.20it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.50it/s]\u001b[A\n",
            " 50% 80/160 [00:32<00:32,  2.45it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.37it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:36,  2.15it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:34,  2.25it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:34,  2.19it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:35,  2.13it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:32,  2.29it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.30it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.51it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.58it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.72it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.94it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.50it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.74it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.85it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.64it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.45it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:23,  2.63it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.74it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.73it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.47it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.17it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.06it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.06it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.29it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.42it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.23it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.23it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:25,  2.00it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.92it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.82it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.01it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.22it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.35it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.46it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.24it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.44it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.29it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.50it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.45it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.53it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.69it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.39it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.38it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.22it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.50it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.67it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.33it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.28it/s]\u001b[A\n",
            " 86% 138/160 [00:57<00:09,  2.26it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:09,  2.32it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:08,  2.47it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.42it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.49it/s]\u001b[A\n",
            " 89% 143/160 [00:59<00:07,  2.35it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.61it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.66it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.55it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:01<00:04,  2.45it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:02<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.24it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.17it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.31it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.42it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.06it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.05it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.04it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.00010504162491997704, 'eval_runtime': 67.801, 'eval_samples_per_second': 2.36, 'eval_steps_per_second': 2.36, 'eval_rewards/chosen': 1.3873865604400635, 'eval_rewards/rejected': -9.657569885253906, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 11.04495620727539, 'eval_logps/chosen': -198.73770141601562, 'eval_logps/rejected': -281.63165283203125, 'eval_logits/chosen': -1.7568950653076172, 'eval_logits/rejected': -1.7217884063720703, 'epoch': 7.15}\n",
            "100% 100/100 [27:44<00:00, 10.06s/it]\n",
            "100% 160/160 [01:07<00:00,  2.19it/s]\u001b[A\n",
            "{'train_runtime': 1665.0747, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.06, 'train_loss': 0.17522237513941946, 'epoch': 7.15}\n",
            "100% 100/100 [27:45<00:00, 16.65s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =     7.1481\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =     0.1752\n",
            "  train_runtime            = 0:27:45.07\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      0.721\n",
            "  train_steps_per_second   =       0.06\n",
            "\u001b[32m2025-12-21 06:36:16.134\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1665.0747, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.06, 'total_flos': 0.0, 'train_loss': 0.17522237513941946, 'epoch': 7.148148148148148, 'train_samples': 500}\u001b[0m\n",
            "\u001b[32m2025-12-21 06:36:16.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m483\u001b[0m - \u001b[1mSaving model checkpoint to outputs-dpo-v1\u001b[0m\n",
            "\u001b[32m2025-12-21 06:36:17.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 160/160 [01:07<00:00,  2.36it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     7.1481\n",
            "  eval_logits/chosen      =    -1.7569\n",
            "  eval_logits/rejected    =    -1.7218\n",
            "  eval_logps/chosen       =  -198.7377\n",
            "  eval_logps/rejected     =  -281.6317\n",
            "  eval_loss               =     0.0001\n",
            "  eval_rewards/accuracies =        1.0\n",
            "  eval_rewards/chosen     =     1.3874\n",
            "  eval_rewards/margins    =     11.045\n",
            "  eval_rewards/rejected   =    -9.6576\n",
            "  eval_runtime            = 0:01:08.05\n",
            "  eval_samples            =        500\n",
            "  eval_samples_per_second =      2.351\n",
            "  eval_steps_per_second   =      2.351\n",
            "\u001b[32m2025-12-21 06:37:25.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m497\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.00010504162491997704, 'eval_runtime': 68.0507, 'eval_samples_per_second': 2.351, 'eval_steps_per_second': 2.351, 'eval_rewards/chosen': 1.3873865604400635, 'eval_rewards/rejected': -9.657569885253906, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 11.04495620727539, 'eval_logps/chosen': -198.73770141601562, 'eval_logps/rejected': -281.63165283203125, 'eval_logits/chosen': -1.7568950653076172, 'eval_logits/rejected': -1.7217884063720703, 'epoch': 7.148148148148148, 'eval_samples': 500}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache \\\n",
        "    --optim adamw_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SrmHBsZ2acp4",
        "outputId": "dd4db8f8-99e6-4796-e0c1-c596299460c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Dec 21 06:36 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 21 06:36 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 21 06:36 added_tokens.json\n",
            "-rw-r--r-- 1 root root  767 Dec 21 06:37 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 06:36 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 06:36 \u001b[0m\u001b[01;34mcheckpoint-100\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 21 06:22 \u001b[01;34mcheckpoint-50\u001b[0m/\n",
            "-rw-r--r-- 1 root root  572 Dec 21 06:37 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 06:36 merges.txt\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 06:36 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 21 06:08 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Dec 21 06:36 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Dec 21 06:36 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  57K Dec 21 06:36 trainer_state.json\n",
            "-rw-r--r-- 1 root root 6.7K Dec 21 06:36 training_args.bin\n",
            "-rw-r--r-- 1 root root  229 Dec 21 06:36 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 21 06:36 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4680wZ7jacp4"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MqaDTbVFacp4"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "M5vt9R7wacp4",
        "outputId": "511b3e6c-22a0-413f-9f7a-18beb7cd617f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 06:40:45.494878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766299245.521486   22250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766299245.527558   22250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766299245.542954   22250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299245.542981   22250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299245.542996   22250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299245.543001   22250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 06:40:45.547538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-sft', tokenizer_path=None, lora_model='outputs-dpo-v1', resize_emb=False, output_dir='merged-dpo/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-sft\n",
            "LoRA model: outputs-dpo-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The tokenizer you are loading from 'merged-sft' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-dpo/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VVPIUseVacp4",
        "outputId": "acfbd673-ae06-43b0-9344-386173722789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 21 06:40 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Dec 21 06:40 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Dec 21 06:40 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 21 06:40 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 21 06:40 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 21 06:41 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 21 06:40 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Dec 21 06:40 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 21 06:40 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 21 06:40 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8R4CpucKacp4",
        "outputId": "e566a9ff-ad1c-43de-ea9d-6688b99ae479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aR9QFk8Cacp4"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "o5HL5tDmacp4"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "uEjKGHezacp4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_3iv6dX1acp4"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "cQMSV01Yacp4",
        "outputId": "98137f41-bf39-4985-a2b7-bf3e8f2ae79f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-21 06:43:01.595726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766299381.615785   22842 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766299381.621872   22842 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766299381.637029   22842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299381.637055   22842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299381.637059   22842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766299381.637062   22842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-21 06:43:01.641875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-dpo', lora_model='', tokenizer_path=None, system_prompt='', stop_str='', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
            "The tokenizer you are loading from 'merged-dpo' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Qwen2TokenizerFast(name_or_path='merged-dpo', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "Starting inference.\n",
            "Generating outputs:   0% 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "===\n",
            "Input: 介绍下北京\n",
            "Output: 北京是一個雄偉而美麗的首都，坐落在中國的北部，靠近天山和長江。它以其悠久的历史、文化和美麗的生態而聞名。北京是一個多元文化的城市，有精美的文物、歷史古蹟和文化遺產，並擁有豐富的飲食選擇。它是一個令人難忘的城市，以其廣泛的文化、歷史和自然風光而聞名。\n",
            "\n",
            "===\n",
            "Input: 乙肝和丙肝的区别？\n",
            "Output: 乙肝（乙型肝炎）和丙型肝炎是两种不同的病毒性肝炎，它们之间存在主要的区别。\n",
            "\n",
            "**乙型肝炎**\n",
            "- **传播途径**：主要通过血液、精液、阴道分泌物、授受精卵或母婴传播。未经消毒的注射器也可能传播。\n",
            "- **症状**：初期可能不明显，但可伴有疲劳、食欲减退、恶心、呕吐、腹痛和腹泻。严重的病例可能会出现黄疽（皮肤和眼睛发黄）。\n",
            "- **诊断**：通常通过血液检测，尤其是在暴露于血液或其他体液后。抗体测试可以提供阳性结果，但不能单独用于诊断。\n",
            "- **治疗**：目前没有根治病毒性肝炎的方法，但可以使用抗病毒药物，尤其是对于有高风险的行为或高传染性的个体。\n",
            "\n",
            "**丙型肝炎**\n",
            "- **传播途径**：主要通过血液、精液、阴道分泌物、授受精卵或母婴传播。性行为、共用注射器也可能传播。\n",
            "- **症状**：同样，早期症状较轻，可能不明显。可能会有疲劳、食欲减退、恶心、呕吐、腹痛和腹泻。严重的病例可能会出现黄疽。\n",
            "- **诊断**：通常通过血液检测，尤其是在暴露于血液或其他体液后。抗体测试可以提供阳性结果，但不能单独用于诊断。\n",
            "- **治疗**：目前没有根治病毒性肝炎的方法，但可以使用抗病毒药物，尤其是对于有高风险的行为或高传染性的个体。\n",
            "\n",
            "总结来说，乙型肝炎主要通过血液传播，而丙型肝炎主要通过性行为传播。它们有不同的传播途径和症状，但都可能发展成慢性肝炎。了解它们的主要区别对于预防和管理它们至关重要。\n",
            "\n",
            "Generating outputs: 100% 1/1 [00:16<00:00, 16.05s/it]\n",
            "Saved to ./predictions_result.jsonl, size: 2\n"
          ]
        }
      ],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NQPLg1w-acp4"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIvbReSracp4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}